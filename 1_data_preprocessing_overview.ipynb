{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 17534/31316 [00:00<00:00, 174066.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 61989\n",
      "After:  31316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31316/31316 [00:00<00:00, 210732.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31316\n",
      "31316\n",
      "31316\n",
      "31316\n",
      "31316\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\ricardo/Github/trump_taiwan_tweets_analysis/datasets/1_tweet_data_overview.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5babbcc9c42a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m output_data.to_csv(\"~/Github/trump_taiwan_tweets_analysis/datasets/1_tweet_data_overview.csv\", \n\u001b[1;32m---> 75\u001b[1;33m                    sep='\\t', index=None)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\normal\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[0;32m   3202\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3203\u001b[0m         )\n\u001b[1;32m-> 3204\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3206\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\normal\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    186\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m                 \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompression_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m             )\n\u001b[0;32m    190\u001b[0m             \u001b[0mclose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\normal\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 428\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    429\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m             \u001b[1;31m# No explicit encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\ricardo/Github/trump_taiwan_tweets_analysis/datasets/1_tweet_data_overview.csv'"
     ]
    }
   ],
   "source": [
    "# Read in the data\n",
    "data = pd.read_csv(\"./datasets/0_tweet_text_merged_tab.csv\", sep = \"\\t\")\n",
    "\n",
    "# drop duplicates rows\n",
    "# remove duplicates rows\n",
    "print('Before:', len(data))\n",
    "data.drop_duplicates(inplace=True)\n",
    "print('After: ', len(data))\n",
    "\n",
    "# remeber to remove the row after checking the code\n",
    "# data = data.iloc[:50, :]\n",
    "\n",
    "is_retweet_list = []\n",
    "retweet_account_list = []\n",
    "tweet_text_list = []\n",
    "original_text_list = []\n",
    "retweet_text_list = []\n",
    "\n",
    "for i, j in enumerate(tqdm(data.loc[:, 'Tweet'])):\n",
    "    tweet_text_list.append(j)\n",
    "    tweet_split_list = str(j).split(\" \")\n",
    "    # check if the tweet is retweeted\n",
    "    if \"RT\" in tweet_split_list:\n",
    "        is_retweet_list.append(1)\n",
    "        # catch the index of RT\n",
    "        RT_index = tweet_split_list.index('RT')\n",
    "        # original text\n",
    "        original_text = \"\"\n",
    "        for a, b in enumerate(tweet_split_list[:RT_index]):\n",
    "            original_text += \" \" + b\n",
    "        original_text_list.append(original_text)\n",
    "        # Retweeted Text\n",
    "        retweeted_text = \"\"\n",
    "        account_grep = False\n",
    "        for a, b in enumerate(tweet_split_list[RT_index:]):\n",
    "            if len(str(b)) >= 1:\n",
    "                # grep account\n",
    "                if str(b)[0] == \"@\" and account_grep == False:\n",
    "                    account_grep = True\n",
    "                    # eliminate \":\" synbol\n",
    "                    if str(b)[-1] == \":\":\n",
    "                        retweet_account_list.append(b[1:-1])\n",
    "                    else:\n",
    "                        retweet_account_list.append(b[1:])\n",
    "                # do not append 'RT' text\n",
    "                elif b == \"RT\":\n",
    "                    pass\n",
    "                else:\n",
    "                    retweeted_text += \" \" + str(b) \n",
    "        if account_grep == False:\n",
    "            retweet_account_list.append(\"\")\n",
    "        retweet_text_list.append(retweeted_text)\n",
    "    else:\n",
    "        is_retweet_list.append(0)\n",
    "        retweet_account_list.append(\"\")\n",
    "        original_text_list.append(j)\n",
    "        retweet_text_list.append(\"\")\n",
    "\n",
    "print(len(is_retweet_list))\n",
    "print(len(retweet_account_list))\n",
    "print(len(tweet_text_list))\n",
    "print(len(original_text_list))\n",
    "print(len(retweet_text_list))\n",
    "        \n",
    "\n",
    "output_data = pd.DataFrame({\n",
    "    'is_RT':is_retweet_list,\n",
    "    'retweet_from':retweet_account_list,\n",
    "    'whole_tweet_text':tweet_text_list,\n",
    "    'original_text':original_text_list,\n",
    "    'retweet_text':retweet_text_list\n",
    "})\n",
    "        \n",
    "output_data.to_csv(\"./datasets/1_tweet_data_overview.csv\", \n",
    "                   sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31316\n",
      "30567\n",
      "749 \n",
      "\n",
      "30567\n",
      "741 \n",
      "\n",
      "614\n"
     ]
    }
   ],
   "source": [
    "# consider original text and retweet data sperately\n",
    "output_data = output_data.sort_values(by=['is_RT'], ascending=False)\n",
    "print(len(output_data))\n",
    "\n",
    "original_text_data = output_data[output_data['is_RT'] == 0]\n",
    "print(len(original_text_data))\n",
    "retweet_text_data = output_data[output_data['is_RT'] == 1]\n",
    "print(len(retweet_text_data), '\\n')\n",
    "\n",
    "# remove duplicates of both dataframe\n",
    "original_text_data = original_text_data.drop_duplicates(subset='original_text')\n",
    "original_text_data.sort_values(by=['original_text'])\n",
    "print(len(original_text_data))\n",
    "retweet_text_data = retweet_text_data.drop_duplicates(subset='retweet_text')\n",
    "retweet_text_data.sort_values(by=['retweet_text'])\n",
    "print(len(retweet_text_data), '\\n')\n",
    "\n",
    "# man defined duplicates search, elastic similarity threshold = 0.4\n",
    "retweet_text_data.to_csv(\"~/Github/trump_taiwan_tweets_analysis/datasets/2_retweet_text_without_elastic_removed.csv\", \n",
    "                   sep='\\t', index=None)\n",
    "for i, j in enumerate(retweet_text_data.iloc[:, 4]):\n",
    "    content_list = j.split(\" \")\n",
    "    for a, b in enumerate(retweet_text_data.iloc[(i+1):, 4]):\n",
    "        tocheck_content_list = b.split(\" \")\n",
    "        # checking procedure\n",
    "        similar_ratio = (len(content_list + tocheck_content_list) - len(list(set(content_list + tocheck_content_list)))) / len(content_list + tocheck_content_list) \n",
    "        if similar_ratio >= 0.40:\n",
    "#             print(similar_ratio, a, b)\n",
    "            retweet_text_data = retweet_text_data[retweet_text_data['retweet_text'] != b]\n",
    "print(len(retweet_text_data))\n",
    "            \n",
    "\n",
    "# export the datas\n",
    "original_text_data.to_csv(\"~/Github/trump_taiwan_tweets_analysis/datasets/3_original_text.csv\",\n",
    "                          sep='\\t', index=None)\n",
    "original_text_data.loc[:, 'original_text'].to_csv(\"~/Github/trump_taiwan_tweets_analysis/datasets/4_original_text_only.csv\",\n",
    "                          sep='\\t', index=None)\n",
    "retweet_text_data.to_csv(\"~/Github/trump_taiwan_tweets_analysis/datasets/5_retweet_text.csv\", \n",
    "                   sep='\\t', index=None)\n",
    "retweet_text_data.loc[:, 'retweet_text'].to_csv(\"~/Github/trump_taiwan_tweets_analysis/datasets/6_retweet_text_only.csv\", \n",
    "                   sep='\\t', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "normal",
   "language": "python",
   "name": "normal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
