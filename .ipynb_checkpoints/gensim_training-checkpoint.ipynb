{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "* change to space splitted basis.\n",
    "* remove duplicate rows and rows that consist too few element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation\n",
    "# revise the data to make it split by space instead of camma\n",
    "file = open(\"./datasets/word_vectors.csv\", mode='r', encoding='utf-8')\n",
    "new_file = open(\"./datasets/tweet_stopwords_removed_space_splitted.csv\", mode='w', encoding='utf-8')\n",
    "\n",
    "for line in file:\n",
    "    line_list = line.split(\",\")\n",
    "    for list_index, list_item in enumerate(line_list):\n",
    "        if list_index in [0]:\n",
    "            pass\n",
    "        else:\n",
    "            new_file.writelines(list_item + \" \")\n",
    "# close the file\n",
    "file.close()\n",
    "new_file.close()\n",
    "\n",
    "# remove rows that length is less than 3\n",
    "tweet_data = pd.read_csv(\"./datasets/tweet_stopwords_removed_space_splitted.csv\")\n",
    "drop_index_list = []\n",
    "\n",
    "# eliminate first blank in each row\n",
    "for i, j in enumerate(tweet_data.iloc[:, 0]):\n",
    "    tweet_data.iloc[i, 0] = str(j)[1:]\n",
    "\n",
    "# remove rows that consist element less than 3\n",
    "for i, j in enumerate(tweet_data.iloc[:, 0]):\n",
    "    line_splitlist = j.split(\" \")\n",
    "    if len(line_splitlist) <= 3:\n",
    "        drop_index_list.append(i)\n",
    "print(len(tweet_data))\n",
    "tweet_data = tweet_data.drop(drop_index_list)\n",
    "\n",
    "# remove duplicate rows\n",
    "print(len(tweet_data))\n",
    "tweet_data = tweet_data.drop_duplicates()\n",
    "\n",
    "print(len(tweet_data))\n",
    "\n",
    "# save as a new csv file\n",
    "tweet_data.to_csv('./datasets/tweet_stopwords_duplicates_short_removed_space_splitted.csv',\n",
    "                 index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Gensim Models - Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from multiprocessing import cpu_count\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train word2vec model by genism, with data: text8\n",
    "dataset = api.load('text8')\n",
    "data = [d for d in dataset]\n",
    "\n",
    "# train word2vec model. Default result vector size = 100\n",
    "model = Word2Vec(data, min_count=0, workers=cpu_count())\n",
    "\n",
    "model.save('./gensim_models/word2vec_model_text8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.corpora.dictionary.Dictionary'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.corpora.dictionary.Dictionary'>\n",
      "<class 'list'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ricardo\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.29335767  0.28058144  1.2209277   0.07219032 -0.05433149 -0.75027883\n",
      " -0.4756051   0.6269139   1.3665643   0.40384582  0.05331578  3.0478833\n",
      " -0.23827238 -0.5766506   1.3480146   0.37904382 -1.0335796  -0.39224458\n",
      " -0.5516785  -2.4345887   1.5142587  -1.7600015   2.6333845   0.81366205\n",
      " -1.789157   -0.38301036 -0.22012158  1.6440449   0.3503979   0.19041426\n",
      "  1.0791503   0.33977607 -1.8347809  -0.22882211  1.3318431   1.7158645\n",
      " -1.1213557  -0.7003229  -0.03556969  1.3839508   1.5685267   1.8462223\n",
      " -0.82925093 -1.0604658   2.3780234   2.0007992   0.2151887   0.37229872\n",
      "  0.3029396   1.9708436   0.02968904  0.2803673  -0.59055823  1.0669053\n",
      " -3.4616804  -1.2851304  -0.08825423 -2.2606418   0.2032655  -1.591786\n",
      "  0.491039   -2.1102717  -1.6018078  -1.8387545  -3.0239727   1.358315\n",
      " -1.4806606   0.38066745  0.6768085  -0.04849679 -0.45524392  0.54454654\n",
      " -0.11318853  1.3164388  -3.3580596  -2.1648653   1.4636595  -0.47797066\n",
      "  0.7874879  -2.0791008   1.3310676  -1.0626041  -0.12971972 -0.24798861\n",
      " -2.6320298   0.83313334 -0.17041117  0.6986863  -0.4694295  -0.2245793\n",
      "  0.69752765 -1.6057047  -2.0271587   0.66034055 -1.7022111   0.6476899\n",
      "  1.5200748  -0.8960348   0.00481058 -0.5784295 ] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ricardo\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:20: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "C:\\Users\\ricardo\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:21: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.1089915   1.0811623   2.3474104  -0.55616593  0.26091388 -0.01272155\n",
      " -0.0440357   0.3969774   0.47158262 -0.27898347  0.37743673  3.552992\n",
      " -0.12242752  0.7295136   2.4407167  -0.7377162  -1.472729   -0.9631364\n",
      "  0.01176887 -4.289425    0.19752944 -1.5961907   2.4035223   0.8474022\n",
      " -2.340774    0.50361663  0.97131807  1.5991626  -0.14529449  0.41963005\n",
      "  1.2242484   0.03783995 -1.5423788  -0.6744898   2.7246504   2.675622\n",
      " -1.3893353   0.03956841 -1.3028386   0.0444745   1.7526083  -0.12947257\n",
      " -1.6173276  -1.1814826   3.217433    3.2263916   0.0722702   0.65972334\n",
      "  0.17476656  0.6797268   0.31193396  0.5565776   0.5646045   1.6013293\n",
      " -2.6540806  -1.4334471  -0.7155246  -1.8489497   0.2911522  -2.9275587\n",
      "  2.1606739  -2.7786162  -1.5501636  -2.6636703  -3.3433623   2.4798617\n",
      " -1.540462   -0.2769955   0.33075973 -1.2379001  -0.5476088   0.18099673\n",
      "  1.7869911   1.3287969  -2.5871916  -1.758952    2.1117709  -2.1903632\n",
      "  1.2822893  -1.5170217   1.2226784   0.18040963  1.3632056  -0.39794776\n",
      " -2.8386009   2.1890593  -1.9895235   0.9819684   0.39257607 -0.8874754\n",
      "  0.5144981  -2.023175   -2.8116887   0.35533327 -0.84268427  0.8008432\n",
      "  1.9569654  -1.2075919   1.2403346   0.24422769]\n"
     ]
    }
   ],
   "source": [
    "# update our model with tweet dataset\n",
    "\n",
    "# create copora dictionary\n",
    "tweet_data = pd.read_csv('./datasets/tweet_stopwords_duplicates_short_removed_space_splitted.csv')\n",
    "data_list = tweet_data.iloc[:, 0].tolist()\n",
    "# tokenize data into words\n",
    "texts = [[text for text in tweet.split()] for tweet in data_list]\n",
    "# create dictionary by copora\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(type(dictionary))\n",
    "print(type(texts))\n",
    "\n",
    "\n",
    "# load text8 word2vec dataset\n",
    "model = Word2Vec.load('./gensim_models/word2vec_model_text8')\n",
    "print(model['media'], '\\n')\n",
    "\n",
    "# update the model with tweets dataset\n",
    "model.build_vocab(texts, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.iter)\n",
    "print(model['media'])\n",
    "\n",
    "# save the model\n",
    "model.save('./gensim_models/word2vec_model_text8_tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.corpora.dictionary.Dictionary'>\n",
      "<class 'list'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ricardo\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\ricardo\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.45001727  1.3712811  -1.4018998  -0.3164949   1.43814    -0.46061862\n",
      "  1.7951301  -1.3389441  -0.3716764   0.14157812  0.18296227  0.89215726\n",
      " -0.55301917 -0.7416799   0.08464012  1.1770823   1.1846385  -1.847964\n",
      "  1.055039   -1.0630884   1.2606027   0.09914911 -0.38686594  0.8529165\n",
      "  0.5230799  -0.09052169  0.62078506 -0.14445329  1.092101    0.69675726\n",
      " -1.0820731   2.1068718  -1.8636477  -0.6896987   0.19457495 -2.0239112\n",
      "  0.3551969  -2.0342014  -1.1496129   0.38238797  0.5103979   0.45153382\n",
      " -0.33121938  0.7251955   1.2568774   0.73351043  1.1155524  -0.03670964\n",
      " -0.7239386   1.0504031  -0.42432594 -0.9509309  -1.1322044  -0.6511916\n",
      " -0.45762008  0.33608925  0.5772421  -0.2521093  -1.3886068   0.8997238\n",
      " -0.995965   -1.2804515   0.8375072   1.2667991   0.4125759   1.1231145\n",
      " -1.3068154   0.41948798 -1.1549307   0.54176843 -1.7674714   1.437072\n",
      " -0.3455198   0.9392271   0.5950211   1.2404965  -0.45126772  0.09859371\n",
      "  0.8435631  -0.17199157 -0.08756505  0.5246988  -0.76778793 -1.2086359\n",
      " -0.45003468  0.7339175   0.62750244 -0.04506618  0.6633076   1.422053\n",
      "  0.28179026 -1.4853638  -0.6134598  -0.02640666 -0.6980098   0.2734103\n",
      "  0.792361    1.3695405  -0.92543596 -0.3013688 ] \n",
      "\n",
      "[ 0.51067126  1.3447353  -1.1074457   0.3249483   0.45884514 -0.5795688\n",
      "  1.5242664  -0.77402025 -0.0435553   0.55572754 -0.2799028   0.42240268\n",
      " -0.51202977 -0.7520289   0.46385974  0.90501994  0.75956416 -1.0433328\n",
      "  1.0819046  -0.8013448   0.7581144   0.28761506 -1.0428174   0.75726044\n",
      " -0.3834507  -0.01328668  0.2275049  -0.2970596   1.0228566   0.9402161\n",
      " -1.6435691   2.0421484  -1.3657305  -0.90187764  0.24734904 -1.7524667\n",
      " -0.13711865 -2.1666055  -0.69930255  0.08801074  0.5243936   0.17493986\n",
      " -0.63864416  0.5285479   1.3113872   0.9505992   1.1699163  -0.15848882\n",
      " -0.6506112   0.5684299   0.06571174 -0.6222705  -0.90152305 -0.90260774\n",
      " -0.54899985 -0.569965   -0.08937683  0.5105367  -0.77297723  1.2176923\n",
      " -0.26167068 -0.7582398   0.2011349   1.0344663  -0.1781115   1.1761981\n",
      " -1.3105496   0.12081477 -0.37272266  0.59765685 -1.5173901   1.4439462\n",
      " -0.66140217  1.1704628   0.8044526   1.860861   -0.23482196 -0.04026641\n",
      "  0.6575749  -0.6494932   0.5468779   0.41953835 -0.4583487  -0.98869526\n",
      "  0.45706192  0.2720081   0.8200873   0.8741202   0.21983716  1.4711266\n",
      " -0.26524186 -0.81016725 -0.5645369   0.10783045 -1.3563552  -0.6918546\n",
      "  1.0760792   0.2752563  -0.87008846 -0.05960485]\n"
     ]
    }
   ],
   "source": [
    "# train a word2vec model with tweet dataset only\n",
    "\n",
    "# create copora dictionary\n",
    "tweet_data = pd.read_csv('./datasets/tweet_stopwords_duplicates_short_removed_space_splitted.csv')\n",
    "data_list = tweet_data.iloc[:, 0].tolist()\n",
    "# tokenize data into words\n",
    "texts = [[text for text in tweet.split()] for tweet in data_list]\n",
    "# create dictionary by copora\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(type(dictionary))\n",
    "print(type(texts))\n",
    "\n",
    "# train word2vec model. Default result vector size = 100\n",
    "model = Word2Vec(texts, min_count=0, workers=cpu_count())\n",
    "print(model['trump'], '\\n')\n",
    "print(model['taiwan'])\n",
    "\n",
    "model.save('./gensim_models/word2vec_model_tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('j', 0.9450893998146057), ('endorses', 0.9430126547813416), ('ttle', 0.9411283731460571), ('ot', 0.9399396181106567), ('telephone', 0.9292389154434204), ('melaniebatley', 0.9265999794006348), ('kpphotography38', 0.9255761504173279), ('sheriffclarke', 0.9241675138473511), ('gladly', 0.9209092259407043), ('bonanza', 0.9208433628082275)] \n",
      "\n",
      "[('call', 0.9987192153930664), ('taiwans', 0.9980147480964661), ('praises', 0.994354784488678), ('considering', 0.9931445121765137), ('operative', 0.9931440353393555), ('jennifer75ar', 0.9929768443107605), ('endorse', 0.9920759797096252), ('headed', 0.9918076992034912), ('congratulatory', 0.9910873174667358), ('risks', 0.9909263253211975)] \n",
      "\n",
      "[('china', 0.9863436222076416), ('eligible', 0.9777646064758301), ('cronies', 0.9762970209121704), ('wants', 0.9760562181472778), ('neither', 0.9753323793411255), ('trap', 0.9737012982368469), ('barack', 0.9723501801490784), ('said', 0.9715273380279541), ('tougher', 0.9713266491889954), ('never', 0.9688180685043335)] \n",
      "\n",
      "[('bentechpro', 0.8147005438804626), ('vpbidens', 0.8145315051078796), ('sovereign', 0.8130879998207092), ('makeamericasafeagain', 0.8107984066009521), ('forward2016', 0.8099133968353271), ('mpeach', 0.8081022500991821), ('beneath', 0.8081014156341553), ('virtue', 0.8080786466598511), ('craigbbannister', 0.8073309659957886), ('immortals', 0.8067179918289185)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the result with only tweets data\n",
    "print(model.wv.most_similar('trump'), '\\n')\n",
    "print(model.wv.most_similar('taiwan'), '\\n')\n",
    "print(model.wv.most_similar('obama'), '\\n')\n",
    "print(model.wv.most_similar('freak'), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim Training - Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create 'tagged document'\n",
    "def create_tagged_document(list_of_list_of_word):\n",
    "    for i, list_of_word in enumerate(list_of_list_of_word):\n",
    "        yield gensim.models.doc2vec.TaggedDocument(list_of_word, [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 'tagged document' dataset of text8\n",
    "# load text8 dataset\n",
    "dataset = api.load('text8')\n",
    "data = [d for d in dataset]\n",
    "\n",
    "# create tagged document of text8\n",
    "training_data_text8 = list(create_tagged_document(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize doc2vec model\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=100)\n",
    "\n",
    "# build the vocabulary\n",
    "model.build_vocab(training_data_text8)\n",
    "\n",
    "# train the Doc2Vec model\n",
    "model.train(training_data_text8, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "model.save('./gensim_models/doc2vec_model_text8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset with text8 and tweets data\n",
    "tweet_data = pd.read_csv('./datasets/tweet_stopwords_duplicates_short_removed_space_splitted.csv')\n",
    "data_list = tweet_data.iloc[:, 0].tolist()\n",
    "\n",
    "# tokenize data into words\n",
    "training_tweet = [[text for text in tweet.split()] for tweet in data_list]\n",
    "\n",
    "# create tagged document with text8 and tweet\n",
    "training_data_text8_tweet = list(create_tagged_document(data + training_tweet))\n",
    "\n",
    "# create tagged document with tweet only\n",
    "training_data_tweet = list(create_tagged_document(training_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model for both text8 and tweets data\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=100)\n",
    "\n",
    "# build the vocabulary\n",
    "model.build_vocab(training_data_text8_tweet)\n",
    "\n",
    "# train the model\n",
    "model.train(training_data_text8_tweet, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "model.save('./gensim_models/doc2vec_model_text8_tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'words'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-2c518490f168>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# build the vocabulary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_tweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\normal\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[1;34m(self, documents, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[0;32m    926\u001b[0m         total_words, corpus_count = self.vocabulary.scan_vocab(\n\u001b[0;32m    927\u001b[0m             \u001b[0mdocuments\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocvecs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 928\u001b[1;33m             \u001b[0mprogress_per\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprogress_per\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    929\u001b[0m         )\n\u001b[0;32m    930\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\normal\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[1;34m(self, documents, corpus_file, docvecs, progress_per, trim_rule)\u001b[0m\n\u001b[0;32m   1123\u001b[0m             \u001b[0mdocuments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTaggedLineDocument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1125\u001b[1;33m         \u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_scan_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocvecs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m         logger.info(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\normal\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m in \u001b[0;36m_scan_vocab\u001b[1;34m(self, documents, docvecs, progress_per, trim_rule)\u001b[0m\n\u001b[0;32m   1052\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdocument_no\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocument\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchecked_string_types\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1054\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1055\u001b[0m                     logger.warning(\n\u001b[0;32m   1056\u001b[0m                         \u001b[1;34m\"Each 'words' should be a list of words (usually unicode strings). \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'words'"
     ]
    }
   ],
   "source": [
    "# create a model for only tweets\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=100)\n",
    "\n",
    "# build the vocabulary\n",
    "model.build_vocab(training_data_tweet)\n",
    "\n",
    "# train the model\n",
    "model.train(training_data_tweet, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "model.save('./gensim_models/doc2vec_model_tweet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "normal",
   "language": "python",
   "name": "normal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
