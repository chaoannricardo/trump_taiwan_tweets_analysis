{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "614\n",
      " The @nytimes was thrilled by Obama talking to Communist dictator Castro, and horrified by Trump talking to the elected l??e\n"
     ]
    }
   ],
   "source": [
    "# f = open('./datasets/6_retweet_text_only.csv', 'r', encoding=\"utf-8\")\n",
    "# tweet = []\n",
    "# for line in f.readlines():\n",
    "#     line = line.strip()\n",
    "#     data = line.split(' ')\n",
    "#     tweet.append(data)\n",
    "# f.close()\n",
    "\n",
    "# f = pd.read_csv('./datasets/4_original_text_only.csv', sep='\\t')\n",
    "f = pd.read_csv('./datasets/6_retweet_text_only.csv', sep='\\t')\n",
    "print(len(f))\n",
    "tweet = f.iloc[:, 0].tolist()\n",
    "print(tweet[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "614\n",
      "['', '@nytimes', 'thrilled', 'obama', 'castro,', 'horrified', 'elected', 'l??e']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ricardo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 去除stop words, link, account\n",
    "nltk.download('stopwords')\n",
    "nltk_stopwords = nltk.corpus.stopwords.words('english')\n",
    "result1 = []\n",
    "\n",
    "for i in range(0, len(tweet)):\n",
    "    tweet_split_list = tweet[i].split(\" \")\n",
    "    words = []\n",
    "    \n",
    "    # convert all words inside the list to be lower case\n",
    "    for j, k in enumerate(tweet_split_list):\n",
    "        tweet_split_list[j] = k.lower()\n",
    "    \n",
    "    for j in range(0, len(tweet_split_list)):\n",
    "        if j == 1:\n",
    "            if '@' not in tweet_split_list[j]:\n",
    "                if 'http' not in tweet_split_list[j]:\n",
    "                    if tweet_split_list[j] not in nltk_stopwords and tweet_split_list[j].lower() not in nltk_stopwords and tweet[i][j].upper() not in nltk_stopwords:\n",
    "                        words.append(tweet_split_list[j])\n",
    "        else:\n",
    "            if tweet[i][j] not in nltk_stopwords and tweet_split_list[j].lower() not in nltk_stopwords and tweet_split_list[j].upper() not in nltk_stopwords:\n",
    "                if 'http' not in tweet_split_list[j]:\n",
    "                    words.append(tweet_split_list[j])\n",
    "    result1.append(words)\n",
    "    \n",
    "print(len(result1))\n",
    "print(result1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "614\n",
      "[['happy', 'year', 'everyone', 'family', 'vacation', 'familytime'], ['trump', 'says', 'taiwain', 'called', 'him', 'trump', 'staffer']]\n"
     ]
    }
   ],
   "source": [
    "# 去除標點符號\n",
    "result2 = []\n",
    "for i in range(0, len(result1)):\n",
    "    words = []\n",
    "    for j in range(0, len(result1[i])):\n",
    "        out = ''.join(c for c in result1[i][j] if c not in string.punctuation)\n",
    "        out = ''.join(x for x in out if ord(x) < 256)\n",
    "        words.append(out)\n",
    "    while '' in words:\n",
    "        words.remove('')\n",
    "    while 'rt' in words:\n",
    "        words.remove('rt')\n",
    "    while '\\uea1d' in words:\n",
    "        words.remove('\\uea1d')\n",
    "    result2.append(words)\n",
    "\n",
    "print(len(result2))\n",
    "print(result2[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "614\n",
      "Index(['is_RT', 'retweet_from', 'whole_tweet_text', 'original_text',\n",
      "       'retweet_text'],\n",
      "      dtype='object')\n",
      "614\n",
      "   is_RT     retweet_from                                   whole_tweet_text  \\\n",
      "0      1    SteveSGoddard  RT @SteveSGoddard: The @nytimes was thrilled b...   \n",
      "1      1   DonaldJTrumpJr  RT @DonaldJTrumpJr: Happy new year everyone. #...   \n",
      "2      1  VoteHillary2016  RT @VoteHillary2016: Trump says Taiwain leader...   \n",
      "3      1   NeonKnight1337  RT @NeonKnight1337: Donald Trump's call with T...   \n",
      "4      1    gatewaypundit  RT @gatewaypundit: The Trump Hotel Waikiki loo...   \n",
      "\n",
      "  original_text                                       retweet_text  \\\n",
      "0           NaN   The @nytimes was thrilled by Obama talking to...   \n",
      "1           NaN   Happy new year everyone. #newyear #family #va...   \n",
      "2           NaN   Trump says Taiwain leader called him, though ...   \n",
      "3           NaN   Donald Trump's call with Taiwan president was...   \n",
      "4           NaN   The Trump Hotel Waikiki looks like a lovely r...   \n",
      "\n",
      "                      stopwords_removed_retweet_text  \n",
      "0  nytimes thrilled obama castro horrified electe...  \n",
      "1     happy year everyone family vacation familytime  \n",
      "2        trump says taiwain called him trump staffer  \n",
      "3               donald call taiwan surprise official  \n",
      "4  trump hotel waikiki looks like resort realdona...  \n"
     ]
    }
   ],
   "source": [
    "# f = open('./datasets/7_retweet_text_only_stopwords_removed.csv', \"w+\", encoding=\"utf-8\")\n",
    "# for i in range(0, len(result2)):\n",
    "#     print(result2[i])\n",
    "#     s = \"\"\n",
    "#     for j in range(0, len(result2[i])):\n",
    "#         s += result2[i][j] + ' '\n",
    "#     f.write(s)\n",
    "#     f.write('\\n')\n",
    "# f.close()\n",
    "\n",
    "content_list = []\n",
    "for i, j in enumerate(result2):\n",
    "    content = \" \".join(str(b) for a, b in enumerate(j))\n",
    "    content_list.append(content)\n",
    "\n",
    "# read in the data with whole information\n",
    "original_data = pd.read_csv(\"./datasets/5_retweet_text.csv\", sep='\\t')\n",
    "# original_data = pd.read_csv(\"./datasets/3_original_text.csv\", sep='\\t')\n",
    "print(len(original_data))\n",
    "print(original_data.columns)\n",
    "\n",
    "# add new column\n",
    "original_data.loc[:, 'stopwords_removed_retweet_text'] = content_list\n",
    "# original_data.loc[:, 'stopwords_removed_original_text'] = content_list\n",
    "print(len(original_data))\n",
    "print(original_data.head())\n",
    "\n",
    "original_data.to_csv(\"./datasets/7_retweet_text_only_stopwords_removed.csv\", \n",
    "                     sep=\"\\t\",\n",
    "                    index=None)\n",
    "# original_data.to_csv(\"./datasets/8_original_text_only_stopwords_removed.csv\", \n",
    "#                      sep=\"\\t\",\n",
    "#                     index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
