{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "* change to space splitted basis.\n",
    "* remove duplicate rows and rows that consist too few element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Gensim Models - Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from multiprocessing import cpu_count\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train word2vec model by genism, with data: text8\n",
    "dataset = api.load('text8')\n",
    "data = [d for d in dataset]\n",
    "\n",
    "# train word2vec model. Default result vector size = 100\n",
    "model = Word2Vec(data, min_count=0, workers=cpu_count())\n",
    "\n",
    "model.save('./gensim_models/word2vec_model_text8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update our model with tweet dataset\n",
    "\n",
    "# create copora dictionary\n",
    "tweet_data = pd.read_csv('./datasets/tweet_stopwords_duplicates_short_removed_space_splitted.csv')\n",
    "data_list = tweet_data.iloc[:, 0].tolist()\n",
    "# tokenize data into words\n",
    "texts = [[text for text in tweet.split()] for tweet in data_list]\n",
    "# create dictionary by copora\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(type(dictionary))\n",
    "print(type(texts))\n",
    "\n",
    "\n",
    "# load text8 word2vec dataset\n",
    "model = Word2Vec.load('./gensim_models/word2vec_model_text8')\n",
    "print(model['media'], '\\n')\n",
    "\n",
    "# update the model with tweets dataset\n",
    "model.build_vocab(texts, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.iter)\n",
    "print(model['media'])\n",
    "\n",
    "# save the model\n",
    "model.save('./gensim_models/word2vec_model_text8_tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a word2vec model with tweet dataset only\n",
    "\n",
    "# create copora dictionary\n",
    "tweet_data = pd.read_csv('./datasets/tweet_stopwords_duplicates_short_removed_space_splitted.csv')\n",
    "data_list = tweet_data.iloc[:, 0].tolist()\n",
    "# tokenize data into words\n",
    "texts = [[text for text in tweet.split()] for tweet in data_list]\n",
    "# create dictionary by copora\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(type(dictionary))\n",
    "print(type(texts))\n",
    "\n",
    "# train word2vec model. Default result vector size = 100\n",
    "model = Word2Vec(texts, min_count=0, workers=cpu_count())\n",
    "print(model['trump'], '\\n')\n",
    "print(model['taiwan'])\n",
    "\n",
    "model.save('./gensim_models/word2vec_model_tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the result with only tweets data\n",
    "print(model.wv.most_similar('trump'), '\\n')\n",
    "print(model.wv.most_similar('taiwan'), '\\n')\n",
    "print(model.wv.most_similar('obama'), '\\n')\n",
    "print(model.wv.most_similar('freak'), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim Training - Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create 'tagged document'\n",
    "def create_tagged_document(list_of_list_of_word):\n",
    "    for i, list_of_word in enumerate(list_of_list_of_word):\n",
    "        yield gensim.models.doc2vec.TaggedDocument(list_of_word, [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 'tagged document' dataset of text8\n",
    "# load text8 dataset\n",
    "dataset = api.load('text8')\n",
    "data = [d for d in dataset]\n",
    "\n",
    "# create tagged document of text8\n",
    "training_data_text8 = list(create_tagged_document(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize doc2vec model\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=100)\n",
    "\n",
    "# build the vocabulary\n",
    "model.build_vocab(training_data_text8)\n",
    "\n",
    "# train the Doc2Vec model\n",
    "model.train(training_data_text8, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "model.save('./gensim_models/doc2vec_model_text8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset with text8 and tweets data\n",
    "tweet_data = pd.read_csv('./datasets/tweet_stopwords_duplicates_short_removed_space_splitted.csv')\n",
    "data_list = tweet_data.iloc[:, 0].tolist()\n",
    "\n",
    "# tokenize data into words\n",
    "training_tweet = [[text for text in tweet.split()] for tweet in data_list]\n",
    "\n",
    "# create tagged document with text8 and tweet\n",
    "training_data_text8_tweet = list(create_tagged_document(data + training_tweet))\n",
    "\n",
    "# create tagged document with tweet only\n",
    "training_data_tweet = list(create_tagged_document(training_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model for both text8 and tweets data\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=100)\n",
    "\n",
    "# build the vocabulary\n",
    "model.build_vocab(training_data_text8_tweet)\n",
    "\n",
    "# train the model\n",
    "model.train(training_data_text8_tweet, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "model.save('./gensim_models/doc2vec_model_text8_tweet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Tagged dataset and gensim models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create 'tagged document'\n",
    "def create_tagged_document(list_of_list_of_word):\n",
    "    for i, list_of_word in enumerate(list_of_list_of_word):\n",
    "        yield gensim.models.doc2vec.TaggedDocument(list_of_word, [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30567\n",
      "Index(['is_RT', 'retweet_from', 'whole_tweet_text', 'original_text',\n",
      "       'retweet_text', 'cleaned_original_text'],\n",
      "      dtype='object')\n",
      "614\n",
      "Index(['is_RT', 'retweet_from', 'whole_tweet_text', 'original_text',\n",
      "       'retweet_text', 'cleaned_retweet_text'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# prepare data\n",
    "original = pd.read_csv(\"./datasets/12_original_text_only_cleaned.csv\", sep='\\t')\n",
    "retweet = pd.read_csv(\"./datasets/11_retweet_text_only_cleaned.csv\", sep='\\t')\n",
    "\n",
    "print(len(original))\n",
    "print(original.columns)\n",
    "print(len(retweet))\n",
    "print(retweet.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create amodel with both text8 and original tweet data\n",
    "\n",
    "# create 'tagged document' dataset of text8\n",
    "# load text8 dataset\n",
    "dataset = api.load('text8')\n",
    "data = [d for d in dataset]\n",
    "\n",
    "# tokenize data into words\n",
    "training_tweet = [[text for text in str(tweet).split()] for tweet in retweet.loc[:, 'cleaned_retweet_text']]\n",
    "# training_tweet = [[text for text in str(tweet).split()] for tweet in original.loc[:, 'cleaned_original_text']]\n",
    "\n",
    "# create tagged document with tweet only\n",
    "training_data_tweet = list(create_tagged_document(training_tweet + data))\n",
    "\n",
    "# create a model for only tweets\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=100)\n",
    "\n",
    "# build the vocabulary\n",
    "model.build_vocab(training_data_tweet)\n",
    "\n",
    "# train the model\n",
    "model.train(training_data_tweet, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "model.save('./gensim_models/doc2vec_model_retweet_new_text8')\n",
    "# model.save('./gensim_models/doc2vec_model_original_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize data into words\n",
    "training_tweet = [[text for text in str(tweet).split()] for tweet in retweet.loc[:, 'cleaned_retweet_text']]\n",
    "# training_tweet = [[text for text in str(tweet).split()] for tweet in original.loc[:, 'cleaned_original_text']]\n",
    "\n",
    "# create tagged document with tweet only\n",
    "training_data_tweet = list(create_tagged_document(training_tweet))\n",
    "\n",
    "# create a model for only tweets\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=100)\n",
    "\n",
    "# build the vocabulary\n",
    "model.build_vocab(training_data_tweet)\n",
    "\n",
    "# train the model\n",
    "model.train(training_data_tweet, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "model.save('./gensim_models/doc2vec_model_retweet_new')\n",
    "# model.save('./gensim_models/doc2vec_model_original_new')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset with Doc2Vec Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 614/614 [00:13<00:00, 45.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   is_RT     retweet_from                                   whole_tweet_text  \\\n",
      "0      1    SteveSGoddard  RT @SteveSGoddard: The @nytimes was thrilled b...   \n",
      "1      1   DonaldJTrumpJr  RT @DonaldJTrumpJr: Happy new year everyone. #...   \n",
      "2      1  VoteHillary2016  RT @VoteHillary2016: Trump says Taiwain leader...   \n",
      "3      1   NeonKnight1337  RT @NeonKnight1337: Donald Trump's call with T...   \n",
      "4      1    gatewaypundit  RT @gatewaypundit: The Trump Hotel Waikiki loo...   \n",
      "\n",
      "  original_text                                       retweet_text  \\\n",
      "0           NaN   The @nytimes was thrilled by Obama talking to...   \n",
      "1           NaN   Happy new year everyone. #newyear #family #va...   \n",
      "2           NaN   Trump says Taiwain leader called him, though ...   \n",
      "3           NaN   Donald Trump's call with Taiwan president was...   \n",
      "4           NaN   The Trump Hotel Waikiki looks like a lovely r...   \n",
      "\n",
      "                                cleaned_retweet_text  feature_0  feature_1  \\\n",
      "0  the nytimes was thrilled by obama talking to c...   1.567425  -0.456293   \n",
      "1  happy new year everyone newyear family vacatio...   1.473637   0.104298   \n",
      "2  trump says taiwain leader called him though ta...   0.983484   0.410189   \n",
      "3  donald trumps call with taiwan president was n...   0.896804   0.161093   \n",
      "4  the trump hotel waikiki looks like a lovely re...   0.718367  -0.128726   \n",
      "\n",
      "   feature_2  feature_3  ...  feature_40  feature_41  feature_42  feature_43  \\\n",
      "0   0.686435  -0.124535  ...   -0.638767    0.419972   -1.225812    0.752463   \n",
      "1  -0.922530   0.332175  ...    0.532002   -0.844501   -0.042387   -0.530082   \n",
      "2   2.034384   0.102665  ...   -0.057597   -0.767277   -0.967954    0.197779   \n",
      "3   1.380735   0.481343  ...    0.709626   -0.704157   -0.252510   -0.262616   \n",
      "4   0.325093  -0.761793  ...    0.550467   -0.202612   -0.221305   -0.159794   \n",
      "\n",
      "   feature_44  feature_45  feature_46  feature_47  feature_48  feature_49  \n",
      "0    0.260167   -1.052362   -2.582944    0.426935   -0.428415   -0.694339  \n",
      "1    0.208882    0.171195   -0.910547   -1.021385   -0.483768   -0.332728  \n",
      "2    0.701325   -0.942907   -1.310759   -0.291038    0.967889    0.448852  \n",
      "3    0.725557    0.024144   -1.382741   -0.272368    0.406654   -0.040898  \n",
      "4    0.734438    0.405943   -2.138988   -0.645602    0.249845   -0.823982  \n",
      "\n",
      "[5 rows x 56 columns]\n",
      "       is_RT   feature_0   feature_1   feature_2   feature_3   feature_4  \\\n",
      "count  614.0  614.000000  614.000000  614.000000  614.000000  614.000000   \n",
      "mean     1.0    0.561092   -0.077331    0.824208   -0.115015   -0.066809   \n",
      "std      0.0    0.675232    0.653464    0.742095    0.592161    0.570221   \n",
      "min      1.0   -1.983145   -2.421257   -1.702402   -2.118935   -2.104964   \n",
      "25%      1.0    0.114118   -0.472560    0.386197   -0.461372   -0.408348   \n",
      "50%      1.0    0.511944   -0.071831    0.806073   -0.106588   -0.097940   \n",
      "75%      1.0    0.964185    0.337148    1.306589    0.242691    0.266724   \n",
      "max      1.0    3.078877    2.120715    2.967767    2.116523    2.331198   \n",
      "\n",
      "        feature_5   feature_6   feature_7   feature_8  ...  feature_40  \\\n",
      "count  614.000000  614.000000  614.000000  614.000000  ...  614.000000   \n",
      "mean    -0.709947    0.210839   -0.318287    0.897194  ...    0.221911   \n",
      "std      0.700321    0.593508    0.596992    0.577396  ...    0.541308   \n",
      "min     -3.254610   -1.435580   -2.508206   -1.107647  ...   -1.751580   \n",
      "25%     -1.118706   -0.118433   -0.712588    0.539948  ...   -0.124026   \n",
      "50%     -0.681525    0.178684   -0.338994    0.857419  ...    0.157983   \n",
      "75%     -0.299896    0.498296    0.042245    1.281988  ...    0.568213   \n",
      "max      1.464966    3.260218    2.172462    2.941060  ...    2.042228   \n",
      "\n",
      "       feature_41  feature_42  feature_43  feature_44  feature_45  feature_46  \\\n",
      "count  614.000000  614.000000  614.000000  614.000000  614.000000  614.000000   \n",
      "mean    -0.094469   -0.315217    0.056943    0.191235   -0.135767   -1.706223   \n",
      "std      0.617691    0.551932    0.578629    0.591779    0.586545    0.684161   \n",
      "min     -2.187984   -2.448681   -1.915698   -2.206758   -2.350919   -3.814592   \n",
      "25%     -0.518738   -0.668930   -0.297411   -0.162354   -0.495254   -2.163127   \n",
      "50%     -0.104002   -0.296072    0.028745    0.150012   -0.098849   -1.657206   \n",
      "75%      0.299926    0.047770    0.372426    0.537003    0.262973   -1.239154   \n",
      "max      2.426544    1.286153    2.208480    2.122656    1.799490    0.270370   \n",
      "\n",
      "       feature_47  feature_48  feature_49  \n",
      "count  614.000000  614.000000  614.000000  \n",
      "mean    -0.606191    0.176770   -0.268111  \n",
      "std      0.564887    0.609861    0.560252  \n",
      "min     -2.964384   -1.738696   -2.864707  \n",
      "25%     -0.939538   -0.171313   -0.616318  \n",
      "50%     -0.605151    0.196656   -0.272741  \n",
      "75%     -0.257210    0.543476    0.047905  \n",
      "max      1.136230    2.076462    1.967782  \n",
      "\n",
      "[8 rows x 51 columns]\n"
     ]
    }
   ],
   "source": [
    "# model = gensim.models.doc2vec.Doc2Vec.load('./gensim_models/doc2vec_model_retweet_new')\n",
    "model = gensim.models.doc2vec.Doc2Vec.load('./gensim_models/doc2vec_model_retweet_new_text8')\n",
    "\n",
    "for i, j in enumerate(tqdm(retweet.loc[:, 'cleaned_retweet_text'])):\n",
    "    element_splitted = str(j).split(\" \")\n",
    "    feature_array = model.infer_vector(element_splitted)\n",
    "    for a, b in enumerate(feature_array):\n",
    "        column_name = 'feature_' + str(a)\n",
    "        retweet.loc[i, column_name] = b\n",
    "\n",
    "print(retweet.head())\n",
    "print(retweet.describe())\n",
    "# retweet.to_csv('./datasets/13_retweet_text_doc2vec_new.csv', sep='\\t', header=True, index=None)\n",
    "retweet.to_csv('./datasets/19_retweet_text_doc2vec_new_text8.csv', sep='\\t', header=True, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30567/30567 [16:37<00:00, 30.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   is_RT  retweet_from                                   whole_tweet_text  \\\n",
      "0      0           NaN  \"@MysticWolf12001: @realDonaldTrump C'mon, you...   \n",
      "1      0           NaN  \"@Chad_Williams91: @realDonaldTrump if you're ...   \n",
      "2      0           NaN  \"@HunterBalthazor: @realDonaldTrump if you ran...   \n",
      "3      0           NaN  \"@kyleraccio: @realDonaldTrump @Vinny_Titone I...   \n",
      "4      0           NaN                @HarryCraig96 @TrumpTowerNY Thanks!   \n",
      "\n",
      "                                       original_text  retweet_text  \\\n",
      "0  \"@MysticWolf12001: @realDonaldTrump C'mon, you...           NaN   \n",
      "1  \"@Chad_Williams91: @realDonaldTrump if you're ...           NaN   \n",
      "2  \"@HunterBalthazor: @realDonaldTrump if you ran...           NaN   \n",
      "3  \"@kyleraccio: @realDonaldTrump @Vinny_Titone I...           NaN   \n",
      "4                @HarryCraig96 @TrumpTowerNY Thanks!           NaN   \n",
      "\n",
      "                               cleaned_original_text  feature_0  feature_1  \\\n",
      "0  mysticwolf12001 cmon you know youre the only o...  -0.768592  -0.792329   \n",
      "1  chadwilliams91 if youre president ill move to ...   0.921706  -1.044010   \n",
      "2  hunterbalthazor if you ran for president you h...   0.593363  -0.937204   \n",
      "3  kyleraccio vinnytitone i think hell lead the p...  -1.371773  -1.320379   \n",
      "4                                harrycraig96 thanks   0.238004  -0.328827   \n",
      "\n",
      "   feature_2  feature_3  ...  feature_40  feature_41  feature_42  feature_43  \\\n",
      "0   0.585194   1.035691  ...    0.314116   -0.414782    0.424944   -1.626707   \n",
      "1   0.845550  -0.481496  ...    1.065640    0.169806    1.559093    2.022515   \n",
      "2   0.891155   1.787635  ...    0.130576    0.632830    0.901615   -0.486742   \n",
      "3   0.048346   0.998821  ...   -0.744296   -0.869827    0.967274    1.104119   \n",
      "4   0.389766   0.018991  ...   -0.054484   -0.065257    0.216145    0.133834   \n",
      "\n",
      "   feature_44  feature_45  feature_46  feature_47  feature_48  feature_49  \n",
      "0   -0.525767    1.040392   -0.418054   -0.899761    0.344677    1.182745  \n",
      "1   -0.505934    0.026736    0.732342    1.288271   -0.895938   -0.214861  \n",
      "2   -0.846205    0.267818    1.795387   -0.094963    0.367216    1.055712  \n",
      "3   -0.399383   -0.907037    0.254770    0.466751    0.778327   -0.327616  \n",
      "4   -0.328141    0.097133    0.276053    0.041791    0.260765    0.255579  \n",
      "\n",
      "[5 rows x 56 columns]\n",
      "         is_RT  retweet_from  retweet_text     feature_0     feature_1  \\\n",
      "count  30567.0           0.0           0.0  30567.000000  30567.000000   \n",
      "mean       0.0           NaN           NaN      0.096971     -0.360446   \n",
      "std        0.0           NaN           NaN      0.851648      0.819463   \n",
      "min        0.0           NaN           NaN     -4.410851     -4.473442   \n",
      "25%        0.0           NaN           NaN     -0.400830     -0.832553   \n",
      "50%        0.0           NaN           NaN      0.107386     -0.338314   \n",
      "75%        0.0           NaN           NaN      0.596454      0.109500   \n",
      "max        0.0           NaN           NaN      4.283805      4.115561   \n",
      "\n",
      "          feature_2     feature_3     feature_4     feature_5     feature_6  \\\n",
      "count  30567.000000  30567.000000  30567.000000  30567.000000  30567.000000   \n",
      "mean       0.825639      0.327620     -0.164400     -0.256674     -0.074276   \n",
      "std        0.750029      0.811215      0.800782      0.820821      0.765274   \n",
      "min       -2.934982     -3.245559     -3.754341     -5.112330     -3.951014   \n",
      "25%        0.370323     -0.144180     -0.643751     -0.721909     -0.518055   \n",
      "50%        0.802094      0.319101     -0.177981     -0.220302     -0.037991   \n",
      "75%        1.275490      0.817539      0.316055      0.216878      0.371302   \n",
      "max        4.921671      5.014572      3.601880      4.006581      3.252676   \n",
      "\n",
      "       ...    feature_40    feature_41    feature_42    feature_43  \\\n",
      "count  ...  30567.000000  30567.000000  30567.000000  30567.000000   \n",
      "mean   ...     -0.099039      0.154990      0.072364      0.171225   \n",
      "std    ...      0.844388      0.765289      0.853346      0.857211   \n",
      "min    ...     -4.620774     -3.494535     -3.938706     -3.635178   \n",
      "25%    ...     -0.588679     -0.289563     -0.418793     -0.320788   \n",
      "50%    ...     -0.079617      0.140078      0.080871      0.178432   \n",
      "75%    ...      0.387959      0.600978      0.568338      0.672341   \n",
      "max    ...      4.010258      3.867847      3.995659      4.061174   \n",
      "\n",
      "         feature_44    feature_45    feature_46    feature_47    feature_48  \\\n",
      "count  30567.000000  30567.000000  30567.000000  30567.000000  30567.000000   \n",
      "mean      -0.502098      0.216883      0.302121      0.412707      0.112738   \n",
      "std        0.752072      0.783794      0.847447      0.807634      0.740814   \n",
      "min       -4.478022     -3.367142     -4.344168     -4.039300     -3.424449   \n",
      "25%       -0.952051     -0.235231     -0.194121     -0.040858     -0.319612   \n",
      "50%       -0.491583      0.204179      0.292118      0.391820      0.122616   \n",
      "75%       -0.063820      0.667211      0.795380      0.895941      0.543887   \n",
      "max        3.020556      4.417666      5.283151      4.291478      3.685886   \n",
      "\n",
      "         feature_49  \n",
      "count  30567.000000  \n",
      "mean       0.488220  \n",
      "std        0.860491  \n",
      "min       -3.935231  \n",
      "25%        0.001423  \n",
      "50%        0.462709  \n",
      "75%        0.987901  \n",
      "max        4.510049  \n",
      "\n",
      "[8 rows x 53 columns]\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec.load('./gensim_models/doc2vec_model_original_new')\n",
    "\n",
    "for i, j in enumerate(tqdm(original.loc[:, 'cleaned_original_text'])):\n",
    "    element_splitted = str(j).split(\" \")\n",
    "    feature_array = model.infer_vector(element_splitted)\n",
    "    for a, b in enumerate(feature_array):\n",
    "        column_name = 'feature_' + str(a)\n",
    "        original.loc[i, column_name] = b\n",
    "\n",
    "print(original.head())\n",
    "print(original.describe())\n",
    "original.to_csv('./datasets/14_original_text_doc2vec_new.csv', sep='\\t', header=True, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim Vector Relations Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy\n",
      " [('birthday', 0.9392526149749756), ('hampshire', 0.9201045036315918), ('x1f1fax1f1f8', 0.8853327035903931), ('donaldjtrumpjr', 0.8516042232513428), ('simply', 0.8400300741195679), ('thank', 0.8388058543205261), ('peace', 0.8381062150001526), ('nevada', 0.8368520736694336), ('primary', 0.8350822925567627), ('incredible', 0.832456111907959)] \n",
      "\n",
      "nytimes\n",
      " [('dnc', 0.9539414644241333), ('foolish', 0.9511896371841431), ('taiwanese', 0.9431979656219482), ('interest', 0.9420510530471802), ('blasts', 0.9390931725502014), ('conflict', 0.9382774829864502), ('means', 0.9286386370658875), ('former', 0.9256524443626404), ('hysterical', 0.9247620105743408), ('arranged', 0.9226740598678589)] \n",
      "\n",
      "trump\n",
      " [('taiwan', 0.8802579045295715), ('arranged', 0.8785498738288879), ('taiwanese', 0.8737508058547974), ('spoke', 0.8711260557174683), ('msm', 0.8688294887542725), ('telephone', 0.8656235337257385), ('foolish', 0.8620258569717407), ('development', 0.8605897426605225), ('which', 0.8572181463241577), ('taking', 0.8571624159812927)] \n",
      "\n",
      "realdonaldtrump\n",
      " [('taiwan', 0.8802579045295715), ('arranged', 0.8785498738288879), ('taiwanese', 0.8737508058547974), ('spoke', 0.8711260557174683), ('msm', 0.8688294887542725), ('telephone', 0.8656235337257385), ('foolish', 0.8620258569717407), ('development', 0.8605897426605225), ('which', 0.8572181463241577), ('taking', 0.8571624159812927)] \n",
      "\n",
      "obama\n",
      " [('sells', 0.9508646726608276), ('billion', 0.9385319948196411), ('sold', 0.9181928038597107), ('weapons', 0.9126449823379517), ('183', 0.9081997871398926), ('arms', 0.9020462036132812), ('worth', 0.9002625942230225), ('500', 0.8975414037704468), ('18', 0.8894564509391785), ('iran', 0.8709158897399902)] \n",
      "\n",
      "taiwan\n",
      " [('call', 0.9013136625289917), ('conflict', 0.8991833925247192), ('conversation', 0.8942769765853882), ('trumps', 0.891249418258667), ('isnt', 0.8903273344039917), ('fiasco', 0.8893347978591919), ('development', 0.8880508542060852), ('trump', 0.8802579641342163), ('phone', 0.8768693804740906), ('goes', 0.8762312531471252)] \n",
      "\n",
      "china\n",
      " [('speaking', 0.9140967726707458), ('off', 0.9117621779441833), ('rift', 0.9077248573303223), ('complaint', 0.8984481692314148), ('major', 0.8983495235443115), ('pissing', 0.897532045841217), ('protest', 0.8975266814231873), ('lodges', 0.8935257196426392), ('diplomatic', 0.893192708492279), ('lodged', 0.8909904956817627)] \n",
      "\n",
      "beijing\n",
      " [('controversial', 0.9776982665061951), ('ap', 0.9565733075141907), ('yi', 0.9537160396575928), ('infuriate', 0.953281044960022), ('ingwen', 0.9498854875564575), ('history', 0.9488028287887573), ('sparks', 0.9461030960083008), ('tsai', 0.9458901882171631), ('between', 0.9412098526954651), ('hopes', 0.9408260583877563)] \n",
      "\n",
      "cnn\n",
      " [('hbo', 0.9669398665428162), ('foxandfriends', 0.8743792772293091), ('dallas', 0.8705385327339172), ('columbus', 0.8665788173675537), ('foxnews', 0.8587372303009033), ('tonight', 0.841569721698761), ('rally', 0.8404409885406494), ('danscavino', 0.8386516571044922), ('national', 0.8355013132095337), ('friend', 0.8308085203170776)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# checking words similarities\n",
    "\n",
    "model = gensim.models.doc2vec.Doc2Vec.load('./gensim_models/doc2vec_model_retweet_new')\n",
    "\n",
    "print('happy\\n', model.wv.most_similar('happy'), '\\n')\n",
    "print('nytimes\\n', model.wv.most_similar('nytimes'), '\\n')\n",
    "print('trump\\n', model.wv.most_similar('trump'), '\\n')\n",
    "print('realdonaldtrump\\n', model.wv.most_similar('trump'), '\\n')\n",
    "print('obama\\n', model.wv.most_similar('obama'), '\\n')\n",
    "print('taiwan\\n', model.wv.most_similar('taiwan'), '\\n')\n",
    "print('china\\n', model.wv.most_similar('china'), '\\n')\n",
    "print('beijing\\n', model.wv.most_similar('beijing'), '\\n')\n",
    "print('cnn\\n', model.wv.most_similar('cnn'), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.20089632 -0.43371838  0.40903682  0.5232539  -0.09847506  0.06498223\n",
      " -0.5256974   0.06251621 -0.67933595  0.61427474 -0.17155817  0.0666979\n",
      " -0.13634105  0.08373062 -0.42614767 -0.04508951 -0.51630783 -0.78576875\n",
      " -0.06757877  0.21523958  0.40046397  0.26988435 -0.2397853   0.38511437\n",
      "  0.01208746 -0.7255458  -0.6818082   0.4965771  -1.0080531  -0.9309631\n",
      "  0.08889045 -0.49097294  0.28477725  0.43024912 -0.70103633  0.41717553\n",
      " -0.54872406 -0.34307778 -0.951142    0.1078896   0.02034028 -0.5034523\n",
      " -0.68035924 -0.7232569  -0.545198    0.6891764  -0.54035807  0.32911068\n",
      "  0.23385102 -0.7544698 ] \n",
      "\n",
      "[-0.04689683 -0.6998818   0.96006435 -0.06521887 -0.19370526 -0.09437462\n",
      " -0.84830284 -0.32293    -1.3422617   0.5378286  -0.6157186   0.1898993\n",
      "  0.01309427  0.19617535 -0.31892866 -1.0963701  -1.3945589  -1.3643827\n",
      "  0.3050841   0.32039866  0.34118     0.58407784 -0.3048922   0.44602215\n",
      "  0.52530587 -1.2686615  -0.70920163  0.3370944  -1.2055398  -1.2726709\n",
      "  0.24273778 -0.42144412  0.55881476  0.59520465 -0.9136528   0.14208713\n",
      " -0.7947347  -0.72292745 -1.0930057  -0.4058047  -0.22125176 -0.14189887\n",
      " -0.5649136  -0.96228915 -1.1450272   0.9294831  -0.65305865 -0.19683869\n",
      "  0.17021245 -0.87900484] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print out vector of words in our dataset\n",
    "\n",
    "print(model['trump'], '\\n')\n",
    "print(model['taiwan'], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.8917866349220276), (183, 0.7322331666946411), (584, 0.658210277557373), (118, 0.63234543800354), (455, 0.6148405075073242), (398, 0.5939778089523315), (31, 0.5921804308891296), (528, 0.5833807587623596), (113, 0.5762577056884766), (166, 0.5754425525665283)]\n",
      "the nytimes was thrilled by obama talking to communist dictator castro and horrified by trump talking to the elected le\n",
      "\n",
      "  ONLY REASON it was \"protocol\" to not talk to Taiwan publicly is Obama is too cowardly &amp; TERRIFIED of offending China.??? \n",
      "\n",
      " Corrupt MSM says President-Elect Trump is not allowed to speak w/ Taiwan because Communist China says so. China does NOT tell??n \n",
      "\n",
      " Donald Trump fell for a ?ittle trick??performed by Taiwan, China says https://t.co/64eU35xFlIcede \n",
      "\n",
      " Story is not: Trump broke diplomatic norms by calling Taiwan. \n",
      "\n",
      "\n",
      "[(38, 0.9044812917709351), (569, 0.6721633672714233), (324, 0.6653714179992676), (393, 0.6373986601829529), (316, 0.6116949319839478), (112, 0.5997557640075684), (577, 0.5979620218276978), (414, 0.5888181924819946), (494, 0.5876942873001099), (286, 0.5865594744682312)]\n",
      "\n",
      "  Incompetence is really the primary threat from the #Trump administration. https://t.co/tbhXqikV6i \n",
      "\n",
      " Live footage from the State Department in wake of Trump Taiwan call https://t.co/DWRx8vRDoj \n",
      "\n",
      " This extract from my book Easternisation gives a flavour of why Trump's Taiwan phone call is so potentially dangerous ht??n \n",
      "\n",
      " .@timkaine's Abortion Flip-Flops: From Valuing The Sanctity of Life --> Pro-Abortion Demagogue #VPdebate https://t.co/aK4061… \n",
      "\n",
      "\n",
      "[(128, 0.8811714053153992), (579, 0.7805080413818359), (134, 0.7792513966560364), (162, 0.7431405186653137), (96, 0.7180862426757812), (398, 0.7160968780517578), (468, 0.7046424150466919), (505, 0.691252589225769), (510, 0.688648521900177), (445, 0.6871014833450317)]\n",
      "\n",
      "  Obama turned his back on Taiwan \n",
      "\n",
      " Trump: 'Is the Boston Killer Eligible for Obama Care to Bring Him Back to Health?' http://t.co/7Z4JlS4Er5 \n",
      "\n",
      " Unlike Obama, Trump isn't going to bow down to foreign leaders or begin his presidency displaying signs of weakness. \n",
      "\n",
      " Obama? been so dignified about a peaceful transition of power?地nd Trump just stabbed him in the back by calling Taiwan??n??ti \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reference: https://stackoverflow.com/questions/42781292/doc2vec-get-most-similar-documents\n",
    "word_tokens = \"the nytimes was thrilled by obama talking to communist dictator castro and horrified by trump talking to the elected le\".split(\" \")\n",
    "token_vector = model.infer_vector(word_tokens)\n",
    "sims = model.docvecs.most_similar([token_vector])\n",
    "print(sims)\n",
    "\n",
    "# print out the original text\n",
    "data = pd.read_csv(\"./datasets/13_retweet_text_doc2vec_new.csv\", sep='\\t')\n",
    "print(\"the nytimes was thrilled by obama talking to communist dictator castro and horrified by trump talking to the elected le\")\n",
    "print('\\n', data.loc[183, 'retweet_text'], '\\n')\n",
    "print(data.loc[584, 'retweet_text'], '\\n')\n",
    "print(data.loc[118, 'retweet_text'], '\\n')\n",
    "print(data.loc[455, 'retweet_text'], '\\n\\n')\n",
    "\n",
    "\n",
    "word_tokens = \"incompetence is really the primary threat from the trump administration\".split(\" \")\n",
    "token_vector = model.infer_vector(word_tokens)\n",
    "sims = model.docvecs.most_similar([token_vector])\n",
    "print(sims)\n",
    "\n",
    "# print out the original text\n",
    "data = pd.read_csv(\"./datasets/13_retweet_text_doc2vec_new.csv\", sep='\\t')\n",
    "print('\\n', data.loc[38, 'retweet_text'], '\\n')\n",
    "print(data.loc[569, 'retweet_text'], '\\n')\n",
    "print(data.loc[40, 'retweet_text'], '\\n')\n",
    "print(data.loc[324, 'retweet_text'], '\\n\\n')\n",
    "\n",
    "\n",
    "word_tokens = \"obama turned his back on taiwan\".split(\" \")\n",
    "token_vector = model.infer_vector(word_tokens)\n",
    "sims = model.docvecs.most_similar([token_vector])\n",
    "print(sims)\n",
    "\n",
    "# print out the original text\n",
    "data = pd.read_csv(\"./datasets/13_retweet_text_doc2vec_new.csv\", sep='\\t')\n",
    "print('\\n', data.loc[128, 'retweet_text'], '\\n')\n",
    "print(data.loc[579, 'retweet_text'], '\\n')\n",
    "print(data.loc[134, 'retweet_text'], '\\n')\n",
    "print(data.loc[162, 'retweet_text'], '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./datasets/10_original_text_doc2vec.csv', sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "normal",
   "language": "python",
   "name": "normal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
