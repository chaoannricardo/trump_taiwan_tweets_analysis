{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "* change to space splitted basis.\n",
    "* remove duplicate rows and rows that consist too few element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Gensim Models - Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from multiprocessing import cpu_count\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train word2vec model by genism, with data: text8\n",
    "dataset = api.load('text8')\n",
    "data = [d for d in dataset]\n",
    "\n",
    "# train word2vec model. Default result vector size = 100\n",
    "model = Word2Vec(data, min_count=0, workers=cpu_count())\n",
    "\n",
    "model.save('./gensim_models/word2vec_model_text8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update our model with tweet dataset\n",
    "\n",
    "# create copora dictionary\n",
    "tweet_data = pd.read_csv('./datasets/tweet_stopwords_duplicates_short_removed_space_splitted.csv')\n",
    "data_list = tweet_data.iloc[:, 0].tolist()\n",
    "# tokenize data into words\n",
    "texts = [[text for text in tweet.split()] for tweet in data_list]\n",
    "# create dictionary by copora\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(type(dictionary))\n",
    "print(type(texts))\n",
    "\n",
    "\n",
    "# load text8 word2vec dataset\n",
    "model = Word2Vec.load('./gensim_models/word2vec_model_text8')\n",
    "print(model['media'], '\\n')\n",
    "\n",
    "# update the model with tweets dataset\n",
    "model.build_vocab(texts, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.iter)\n",
    "print(model['media'])\n",
    "\n",
    "# save the model\n",
    "model.save('./gensim_models/word2vec_model_text8_tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a word2vec model with tweet dataset only\n",
    "\n",
    "# create copora dictionary\n",
    "tweet_data = pd.read_csv('./datasets/tweet_stopwords_duplicates_short_removed_space_splitted.csv')\n",
    "data_list = tweet_data.iloc[:, 0].tolist()\n",
    "# tokenize data into words\n",
    "texts = [[text for text in tweet.split()] for tweet in data_list]\n",
    "# create dictionary by copora\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(type(dictionary))\n",
    "print(type(texts))\n",
    "\n",
    "# train word2vec model. Default result vector size = 100\n",
    "model = Word2Vec(texts, min_count=0, workers=cpu_count())\n",
    "print(model['trump'], '\\n')\n",
    "print(model['taiwan'])\n",
    "\n",
    "model.save('./gensim_models/word2vec_model_tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the result with only tweets data\n",
    "print(model.wv.most_similar('trump'), '\\n')\n",
    "print(model.wv.most_similar('taiwan'), '\\n')\n",
    "print(model.wv.most_similar('obama'), '\\n')\n",
    "print(model.wv.most_similar('freak'), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim Training - Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create 'tagged document'\n",
    "def create_tagged_document(list_of_list_of_word):\n",
    "    for i, list_of_word in enumerate(list_of_list_of_word):\n",
    "        yield gensim.models.doc2vec.TaggedDocument(list_of_word, [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 'tagged document' dataset of text8\n",
    "# load text8 dataset\n",
    "dataset = api.load('text8')\n",
    "data = [d for d in dataset]\n",
    "\n",
    "# create tagged document of text8\n",
    "training_data_text8 = list(create_tagged_document(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize doc2vec model\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=100)\n",
    "\n",
    "# build the vocabulary\n",
    "model.build_vocab(training_data_text8)\n",
    "\n",
    "# train the Doc2Vec model\n",
    "model.train(training_data_text8, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "model.save('./gensim_models/doc2vec_model_text8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset with text8 and tweets data\n",
    "tweet_data = pd.read_csv('./datasets/tweet_stopwords_duplicates_short_removed_space_splitted.csv')\n",
    "data_list = tweet_data.iloc[:, 0].tolist()\n",
    "\n",
    "# tokenize data into words\n",
    "training_tweet = [[text for text in tweet.split()] for tweet in data_list]\n",
    "\n",
    "# create tagged document with text8 and tweet\n",
    "training_data_text8_tweet = list(create_tagged_document(data + training_tweet))\n",
    "\n",
    "# create tagged document with tweet only\n",
    "training_data_tweet = list(create_tagged_document(training_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model for both text8 and tweets data\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=100)\n",
    "\n",
    "# build the vocabulary\n",
    "model.build_vocab(training_data_text8_tweet)\n",
    "\n",
    "# train the model\n",
    "model.train(training_data_text8_tweet, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "model.save('./gensim_models/doc2vec_model_text8_tweet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Tagged dataset and gensim models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create 'tagged document'\n",
    "def create_tagged_document(list_of_list_of_word):\n",
    "    for i, list_of_word in enumerate(list_of_list_of_word):\n",
    "        yield gensim.models.doc2vec.TaggedDocument(list_of_word, [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30567\n",
      "Index(['is_RT', 'retweet_from', 'whole_tweet_text', 'original_text',\n",
      "       'retweet_text', 'stopwords_removed_original_text'],\n",
      "      dtype='object')\n",
      "614\n",
      "Index(['is_RT', 'retweet_from', 'whole_tweet_text', 'original_text',\n",
      "       'retweet_text', 'stopwords_removed_retweet_text'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# prepare data by concating original and removed stopwords together\n",
    "original = pd.read_csv(\"./datasets/8_original_text_only_stopwords_removed.csv\", sep='\\t')\n",
    "retweet = pd.read_csv(\"./datasets/7_retweet_text_only_stopwords_removed.csv\", sep='\\t')\n",
    "\n",
    "print(len(original))\n",
    "print(original.columns)\n",
    "print(len(retweet))\n",
    "print(retweet.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize data into words\n",
    "# training_tweet = [[text for text in str(tweet).split()] for tweet in retweet.loc[:, 'stopwords_removed_retweet_text']]\n",
    "training_tweet = [[text for text in str(tweet).split()] for tweet in original.loc[:, 'stopwords_removed_original_text']]\n",
    "\n",
    "# create tagged document with tweet only\n",
    "training_data_tweet = list(create_tagged_document(training_tweet))\n",
    "\n",
    "# create a model for only tweets\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=100)\n",
    "\n",
    "# build the vocabulary\n",
    "model.build_vocab(training_data_tweet)\n",
    "\n",
    "# train the model\n",
    "model.train(training_data_tweet, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# model.save('./gensim_models/doc2vec_model_retweet')\n",
    "model.save('./gensim_models/doc2vec_model_original')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset with Doc2Vec Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 614/614 [00:11<00:00, 54.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   is_RT     retweet_from                                   whole_tweet_text  \\\n",
      "0      1    SteveSGoddard  RT @SteveSGoddard: The @nytimes was thrilled b...   \n",
      "1      1   DonaldJTrumpJr  RT @DonaldJTrumpJr: Happy new year everyone. #...   \n",
      "2      1  VoteHillary2016  RT @VoteHillary2016: Trump says Taiwain leader...   \n",
      "3      1   NeonKnight1337  RT @NeonKnight1337: Donald Trump's call with T...   \n",
      "4      1    gatewaypundit  RT @gatewaypundit: The Trump Hotel Waikiki loo...   \n",
      "\n",
      "  original_text                                       retweet_text  \\\n",
      "0           NaN   The @nytimes was thrilled by Obama talking to...   \n",
      "1           NaN   Happy new year everyone. #newyear #family #va...   \n",
      "2           NaN   Trump says Taiwain leader called him, though ...   \n",
      "3           NaN   Donald Trump's call with Taiwan president was...   \n",
      "4           NaN   The Trump Hotel Waikiki looks like a lovely r...   \n",
      "\n",
      "                      stopwords_removed_retweet_text  feature_0  feature_1  \\\n",
      "0  nytimes thrilled obama castro horrified electe...   0.102114  -0.124808   \n",
      "1     happy year everyone family vacation familytime   0.036475  -0.160620   \n",
      "2        trump says taiwain called him trump staffer  -0.105445  -0.018096   \n",
      "3               donald call taiwan surprise official   0.092953   0.028014   \n",
      "4  trump hotel waikiki looks like resort realdona...  -0.062623   0.088097   \n",
      "\n",
      "   feature_2  feature_3  ...  feature_40  feature_41  feature_42  feature_43  \\\n",
      "0   0.217478   0.021824  ...   -0.056146    0.004159   -0.036622    0.090602   \n",
      "1   0.329864   0.012973  ...    0.056109    0.121683    0.083413   -0.010287   \n",
      "2   0.153127  -0.032872  ...    0.072655    0.092670   -0.098058   -0.007427   \n",
      "3  -0.023101  -0.016088  ...    0.093004   -0.019536    0.084817    0.045769   \n",
      "4   0.130430   0.091245  ...    0.069882    0.139344   -0.008893   -0.026550   \n",
      "\n",
      "   feature_44  feature_45  feature_46  feature_47  feature_48  feature_49  \n",
      "0   -0.022705    0.069969   -0.230823   -0.166129   -0.081601   -0.010727  \n",
      "1   -0.102281    0.198471   -0.245273   -0.249155   -0.189985   -0.021279  \n",
      "2   -0.120844    0.013155   -0.074325    0.001916   -0.064294    0.024633  \n",
      "3    0.043315    0.143166   -0.130785    0.032519   -0.084703   -0.073735  \n",
      "4   -0.058734    0.112519   -0.225255   -0.022200   -0.208501    0.005942  \n",
      "\n",
      "[5 rows x 56 columns]\n",
      "       is_RT   feature_0   feature_1   feature_2   feature_3   feature_4  \\\n",
      "count  614.0  614.000000  614.000000  614.000000  614.000000  614.000000   \n",
      "mean     1.0    0.026252   -0.026818    0.155107    0.023951   -0.108802   \n",
      "std      0.0    0.181572    0.195265    0.150643    0.125692    0.116261   \n",
      "min      1.0   -0.919804   -0.831776   -0.630100   -0.881626   -0.682508   \n",
      "25%      1.0   -0.044145   -0.098037    0.092749   -0.018774   -0.160028   \n",
      "50%      1.0    0.019077   -0.021418    0.170813    0.024495   -0.105810   \n",
      "75%      1.0    0.097213    0.030921    0.222640    0.058704   -0.050595   \n",
      "max      1.0    0.799006    0.996590    0.996410    0.723283    0.280610   \n",
      "\n",
      "        feature_5   feature_6   feature_7   feature_8  ...  feature_40  \\\n",
      "count  614.000000  614.000000  614.000000  614.000000  ...  614.000000   \n",
      "mean    -0.136389   -0.000469    0.207619   -0.051380  ...    0.033258   \n",
      "std      0.126427    0.149440    0.111867    0.128144  ...    0.145836   \n",
      "min     -0.708321   -0.550247   -0.251577   -0.592665  ...   -0.741393   \n",
      "25%     -0.188398   -0.067195    0.143422   -0.106993  ...   -0.006090   \n",
      "50%     -0.135373   -0.008697    0.217692   -0.056697  ...    0.043133   \n",
      "75%     -0.074541    0.058286    0.275567   -0.014963  ...    0.085501   \n",
      "max      0.333644    0.824998    0.583358    1.018505  ...    0.712627   \n",
      "\n",
      "       feature_41  feature_42  feature_43  feature_44  feature_45  feature_46  \\\n",
      "count  614.000000  614.000000  614.000000  614.000000  614.000000  614.000000   \n",
      "mean     0.052581   -0.031651    0.078949   -0.033197    0.116080   -0.196513   \n",
      "std      0.133437    0.105845    0.097665    0.121868    0.126294    0.113481   \n",
      "min     -0.506172   -0.601230   -0.216557   -0.783789   -0.695003   -0.612298   \n",
      "25%     -0.009993   -0.076152    0.021136   -0.084747    0.077436   -0.258136   \n",
      "50%      0.058968   -0.033711    0.073744   -0.044813    0.140285   -0.210789   \n",
      "75%      0.116982    0.016965    0.115538    0.006052    0.181906   -0.149923   \n",
      "max      0.661923    0.577973    0.789804    0.641111    0.467944    0.289719   \n",
      "\n",
      "       feature_47  feature_48  feature_49  \n",
      "count  614.000000  614.000000  614.000000  \n",
      "mean    -0.136747   -0.116410   -0.000864  \n",
      "std      0.164521    0.145391    0.112669  \n",
      "min     -0.842783   -0.713896   -0.570860  \n",
      "25%     -0.219112   -0.193280   -0.050812  \n",
      "50%     -0.153781   -0.134854   -0.002998  \n",
      "75%     -0.052491   -0.043881    0.038546  \n",
      "max      0.683763    0.363332    0.607309  \n",
      "\n",
      "[8 rows x 51 columns]\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec.load('./gensim_models/doc2vec_model_retweet')\n",
    "\n",
    "for i, j in enumerate(tqdm(retweet.loc[:, 'stopwords_removed_retweet_text'])):\n",
    "    element_splitted = str(j).split(\" \")\n",
    "    feature_array = model.infer_vector(element_splitted)\n",
    "    for a, b in enumerate(feature_array):\n",
    "        column_name = 'feature_' + str(a)\n",
    "        retweet.loc[i, column_name] = b\n",
    "\n",
    "print(retweet.head())\n",
    "print(retweet.describe())\n",
    "retweet.to_csv('./datasets/10_retweet_text_doc2vec.csv', sep='\\t', header=True, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30567/30567 [11:58<00:00, 42.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   is_RT  retweet_from                                   whole_tweet_text  \\\n",
      "0      0           NaN  \"@MysticWolf12001: @realDonaldTrump C'mon, you...   \n",
      "1      0           NaN  \"@Chad_Williams91: @realDonaldTrump if you're ...   \n",
      "2      0           NaN  \"@HunterBalthazor: @realDonaldTrump if you ran...   \n",
      "3      0           NaN  \"@kyleraccio: @realDonaldTrump @Vinny_Titone I...   \n",
      "4      0           NaN                @HarryCraig96 @TrumpTowerNY Thanks!   \n",
      "\n",
      "                                       original_text  retweet_text  \\\n",
      "0  \"@MysticWolf12001: @realDonaldTrump C'mon, you...           NaN   \n",
      "1  \"@Chad_Williams91: @realDonaldTrump if you're ...           NaN   \n",
      "2  \"@HunterBalthazor: @realDonaldTrump if you ran...           NaN   \n",
      "3  \"@kyleraccio: @realDonaldTrump @Vinny_Titone I...           NaN   \n",
      "4                @HarryCraig96 @TrumpTowerNY Thanks!           NaN   \n",
      "\n",
      "                     stopwords_removed_original_text  feature_0  feature_1  \\\n",
      "0           mysticwolf12001 cmon one beating clinton  -0.016500  -0.005281   \n",
      "1  chadwilliams91 ill move states you are hired t...   0.599302   0.204645   \n",
      "2      hunterbalthazor ran president vote back track  -0.315002   0.056931   \n",
      "3  kyleraccio vinnytitone think hell lead polls l...   0.238759   0.785895   \n",
      "4                                       harrycraig96   0.009090   0.001305   \n",
      "\n",
      "   feature_2  feature_3  ...  feature_40  feature_41  feature_42  feature_43  \\\n",
      "0   0.017283  -0.061644  ...    0.312471   -0.075369    0.321129    0.225441   \n",
      "1   0.212558  -0.769105  ...   -0.141310    0.946372   -0.284246    0.381726   \n",
      "2  -0.262363   0.113746  ...   -0.169697   -0.124078   -0.052634    0.397521   \n",
      "3   1.245007  -0.907808  ...   -1.241264   -1.062548    1.296389    0.560818   \n",
      "4  -0.003960   0.003530  ...    0.008313   -0.007698    0.000524    0.002306   \n",
      "\n",
      "   feature_44  feature_45  feature_46  feature_47  feature_48  feature_49  \n",
      "0   -0.219494    0.368999   -0.154615   -0.438992    0.441048   -0.131179  \n",
      "1   -0.194061    0.297913    0.289606   -0.598268    0.801785    0.753717  \n",
      "2   -0.253011    0.193542   -0.634213   -0.736388    0.920718   -0.184973  \n",
      "3    1.016314    0.239917   -1.198801    0.180058   -0.949203    0.482925  \n",
      "4    0.006871   -0.004862   -0.007273   -0.009166   -0.005889   -0.005700  \n",
      "\n",
      "[5 rows x 56 columns]\n",
      "         is_RT  retweet_from  retweet_text     feature_0     feature_1  \\\n",
      "count  30567.0           0.0           0.0  30567.000000  30567.000000   \n",
      "mean       0.0           NaN           NaN     -0.050722      0.132078   \n",
      "std        0.0           NaN           NaN      0.590149      0.532629   \n",
      "min        0.0           NaN           NaN     -4.050811     -3.143460   \n",
      "25%        0.0           NaN           NaN     -0.366129     -0.153524   \n",
      "50%        0.0           NaN           NaN     -0.053629      0.129313   \n",
      "75%        0.0           NaN           NaN      0.266048      0.412012   \n",
      "max        0.0           NaN           NaN      3.779444      3.112489   \n",
      "\n",
      "          feature_2     feature_3     feature_4     feature_5     feature_6  \\\n",
      "count  30567.000000  30567.000000  30567.000000  30567.000000  30567.000000   \n",
      "mean       0.145817      0.150087      0.003941     -0.017662     -0.280271   \n",
      "std        0.486057      0.568640      0.524258      0.524971      0.528939   \n",
      "min       -2.979858     -3.111488     -3.151244     -3.158544     -2.782561   \n",
      "25%       -0.114510     -0.152205     -0.273686     -0.302731     -0.576888   \n",
      "50%        0.133501      0.159219      0.018465     -0.012264     -0.281921   \n",
      "75%        0.407658      0.460317      0.287257      0.261696      0.010470   \n",
      "max        2.996956      3.882893      3.012997      3.164253      2.477215   \n",
      "\n",
      "       ...    feature_40    feature_41    feature_42    feature_43  \\\n",
      "count  ...  30567.000000  30567.000000  30567.000000  30567.000000   \n",
      "mean   ...     -0.230858      0.028641      0.175560      0.344024   \n",
      "std    ...      0.538438      0.542556      0.533812      0.506200   \n",
      "min    ...     -3.121665     -3.416882     -2.929933     -2.278770   \n",
      "25%    ...     -0.527561     -0.255360     -0.103593      0.058985   \n",
      "50%    ...     -0.213850      0.050013      0.171323      0.342730   \n",
      "75%    ...      0.060681      0.318265      0.469060      0.632641   \n",
      "max    ...      2.885680      2.617330      2.935327      3.431499   \n",
      "\n",
      "         feature_44    feature_45    feature_46    feature_47    feature_48  \\\n",
      "count  30567.000000  30567.000000  30567.000000  30567.000000  30567.000000   \n",
      "mean      -0.050178      0.241428     -0.447282     -0.246633     -0.022376   \n",
      "std        0.496943      0.526010      0.506133      0.518104      0.578812   \n",
      "min       -3.079316     -2.896207     -3.145000     -2.959306     -3.167775   \n",
      "25%       -0.314011     -0.054593     -0.739261     -0.540742     -0.329904   \n",
      "50%       -0.048314      0.234269     -0.437305     -0.248766     -0.025706   \n",
      "75%        0.219617      0.536724     -0.157001      0.038434      0.286566   \n",
      "max        2.709185      3.214388      2.475270      2.355644      4.072074   \n",
      "\n",
      "         feature_49  \n",
      "count  30567.000000  \n",
      "mean      -0.181459  \n",
      "std        0.574683  \n",
      "min       -3.109475  \n",
      "25%       -0.509967  \n",
      "50%       -0.195244  \n",
      "75%        0.126133  \n",
      "max        2.753305  \n",
      "\n",
      "[8 rows x 53 columns]\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec.load('./gensim_models/doc2vec_model_original')\n",
    "\n",
    "for i, j in enumerate(tqdm(original.loc[:, 'stopwords_removed_original_text'])):\n",
    "    element_splitted = str(j).split(\" \")\n",
    "    feature_array = model.infer_vector(element_splitted)\n",
    "    for a, b in enumerate(feature_array):\n",
    "        column_name = 'feature_' + str(a)\n",
    "        original.loc[i, column_name] = b\n",
    "\n",
    "print(original.head())\n",
    "print(original.describe())\n",
    "original.to_csv('./datasets/10_original_text_doc2vec.csv', sep='\\t', header=True, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim Vector Relations Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy\n",
      " [('life', 0.9962131381034851), ('birthday', 0.9940576553344727), ('2016', 0.9935583472251892), ('hi', 0.9930458068847656), ('past', 0.9928525686264038), ('entire', 0.9925838708877563), ('love', 0.9925692081451416), ('wish', 0.992324948310852), ('days', 0.9922604560852051), ('fight', 0.9922025203704834)] \n",
      "\n",
      "nytimes\n",
      " [('chinese', 0.9958356618881226), ('yet', 0.994345486164093), ('tell', 0.9935946464538574), ('re', 0.9932842254638672), ('trick', 0.9932230710983276), ('department', 0.9930613040924072), ('ill', 0.9929087162017822), ('so', 0.9928550720214844), ('goes', 0.9926146268844604), ('plays', 0.9925909042358398)] \n",
      "\n",
      "trump\n",
      " [('weapons', 0.9693093299865723), ('1', 0.9690811634063721), ('takes', 0.9671955108642578), ('directly', 0.966316819190979), ('relations', 0.9662920236587524), ('beijing', 0.9661856889724731), ('protocol', 0.9656442403793335), ('chinas', 0.9648846387863159), ('1979', 0.9646134972572327), ('contacted', 0.9625431895256042)] \n",
      "\n",
      "realdonaldtrump\n",
      " [('weapons', 0.9693093299865723), ('1', 0.9690811634063721), ('takes', 0.9671955108642578), ('directly', 0.966316819190979), ('relations', 0.9662920236587524), ('beijing', 0.9661856889724731), ('protocol', 0.9656442403793335), ('chinas', 0.9648846387863159), ('1979', 0.9646134972572327), ('contacted', 0.9625431895256042)] \n",
      "\n",
      "obama\n",
      " [('wonder', 0.9812735319137573), ('reagan', 0.9808530211448669), ('chinataiwan', 0.9799010753631592), ('takes', 0.9744272232055664), ('usa', 0.9743203520774841), ('sold', 0.9655231237411499), ('years', 0.9653322696685791), ('clueless', 0.965240478515625), ('lied', 0.9644318222999573), ('disaster', 0.9622949957847595)] \n",
      "\n",
      "taiwan\n",
      " [('w', 0.9457716941833496), ('development', 0.9318029880523682), ('possible', 0.9271982908248901), ('caught', 0.9188271760940552), ('fiasco', 0.9148255586624146), ('lying', 0.9088253974914551), ('ties', 0.9080923795700073), ('organization', 0.9061409831047058), ('security', 0.9052594304084778), ('pres', 0.9008255004882812)] \n",
      "\n",
      "china\n",
      " [('lodges', 0.991895854473114), ('protest', 0.989809513092041), ('call', 0.9781961441040039), ('taiwans', 0.9731988310813904), ('complaint', 0.971868097782135), ('diplomatic', 0.9696673154830933), ('breaking', 0.9647144675254822), ('phone', 0.9608157277107239), ('sparks', 0.9498457908630371), ('president', 0.9458166360855103)] \n",
      "\n",
      "beijing\n",
      " [('relations', 0.9930198788642883), ('contacted', 0.992519199848175), ('chinas', 0.9912858009338379), ('ap', 0.990354061126709), ('there', 0.9890565276145935), ('chinese', 0.9882513284683228), ('protocol', 0.9879732131958008), ('protests', 0.9872339963912964), ('donaldtrump', 0.987183153629303), ('speaks', 0.9870570302009583)] \n",
      "\n",
      "cnn\n",
      " [('hbo', 0.9952870607376099), ('mr', 0.9915814995765686), ('talking', 0.9906992316246033), ('spent', 0.9900892972946167), ('times', 0.9897661209106445), ('things', 0.98959881067276), ('lives', 0.9888870716094971), ('gop', 0.9888819456100464), ('endorsement', 0.9885513782501221), ('donaldjtrumpjr', 0.9884389042854309)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# checking words similarities\n",
    "\n",
    "model = gensim.models.doc2vec.Doc2Vec.load('./gensim_models/doc2vec_model_retweet')\n",
    "\n",
    "print('happy\\n', model.wv.most_similar('happy'), '\\n')\n",
    "print('nytimes\\n', model.wv.most_similar('nytimes'), '\\n')\n",
    "print('trump\\n', model.wv.most_similar('trump'), '\\n')\n",
    "print('realdonaldtrump\\n', model.wv.most_similar('trump'), '\\n')\n",
    "print('obama\\n', model.wv.most_similar('obama'), '\\n')\n",
    "print('taiwan\\n', model.wv.most_similar('taiwan'), '\\n')\n",
    "print('china\\n', model.wv.most_similar('china'), '\\n')\n",
    "print('beijing\\n', model.wv.most_similar('beijing'), '\\n')\n",
    "print('cnn\\n', model.wv.most_similar('cnn'), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.07589609 -0.22214681  0.23832229  0.06620979 -0.44298905 -0.33639458\n",
      "  0.13036525  0.36259282 -0.03343709  0.1421779  -0.2008731   0.11348194\n",
      " -0.6292838   0.26848537 -0.49353597  0.5283196   0.06895273 -0.43224335\n",
      "  0.13933642  0.37235594 -0.1362137   0.4131076  -0.00813438  0.27997336\n",
      " -0.16048741 -0.26093954 -0.25661844 -0.32493547  0.07078141 -0.01224881\n",
      " -0.38897976  0.30024302  0.28150973 -0.0953938   0.36339694 -0.36434603\n",
      "  0.24455465 -0.14273247 -0.19143683 -0.38356858 -0.0894326   0.01000968\n",
      " -0.04487084  0.10003866 -0.05941181  0.07949563 -0.3191504  -0.0888838\n",
      " -0.00231079 -0.05371438] \n",
      "\n",
      "[ 0.27641258  0.06169457 -0.02479559 -0.0086255  -0.36096796 -0.49952307\n",
      "  0.17952746  0.3778566   0.05496087  0.23062617 -0.24573363  0.36482796\n",
      " -0.9389639   0.10517863 -0.38666457  0.47962666  0.11240822 -0.40039933\n",
      "  0.24218293  0.45520246  0.0089858   0.5151568  -0.04509442  0.31782252\n",
      " -0.08131584 -0.26947916 -0.26699954 -0.34306556 -0.02701423 -0.07783069\n",
      " -0.5649129   0.2518105   0.2713349  -0.3670182   0.23605415 -0.32296202\n",
      "  0.3536418   0.01455595 -0.20203114 -0.62817705  0.02538584 -0.01682964\n",
      "  0.10765456  0.08532665 -0.00827823  0.20407733 -0.12337056  0.18139718\n",
      " -0.17339405 -0.2685136 ]\n"
     ]
    }
   ],
   "source": [
    "# print out vector of words in our dataset\n",
    "\n",
    "print(model['trump'], '\\n')\n",
    "print(model['taiwan'], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(14, 0.9203613996505737), (13, 0.9139428734779358), (240, 0.9054841995239258), (1, 0.8906506299972534), (330, 0.8876212239265442), (97, 0.884807825088501), (169, 0.8805535435676575), (372, 0.8759569525718689), (357, 0.8747690320014954), (386, 0.872344434261322)]\n",
      "\n",
      "  2016 has been one of the most eventful and exciting years of my life. I wish you peace, joy, love and laughter. Happy New… \n",
      "\n",
      " Happy New Year + God's blessings to you all. Looking forward to incredible things in 2017! @realDonaldTrump will Make America… \n",
      "\n",
      " HAPPY 241st BIRTHDAY to the @USArmy! THANK YOU! https://t.co/mXsxkfcstC \n",
      "\n",
      " Happy new year everyone. #newyear #family #vacation #familytime https://t.co/u9fJIKNoZq \n",
      "\n",
      "\n",
      "[(440, 0.9283744096755981), (559, 0.9247827529907227), (576, 0.9166402816772461), (445, 0.9062837362289429), (41, 0.9006775617599487), (515, 0.8985196948051453), (57, 0.8975746035575867), (578, 0.8971558809280396), (217, 0.8946325778961182), (586, 0.891290009021759)]\n",
      "\n",
      "  Trump's talk with Taiwan president upended 40 years of US foreign policy. But perhaps it helped his business there. https://??S \n",
      "\n",
      " #Trump has breeched decades of #USA foreign policy towards #China &amp; lied about the circumstances. Bodes well!! #China #Taiwan \n",
      "\n",
      " Has controversial call between President-elect Trump &amp; Taiwan's President overturned decades of diplomatic protocol? https:??i \n",
      "\n",
      " Obama borrows $500 billion from China in 8 years. Media is silent \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reference: https://stackoverflow.com/questions/42781292/doc2vec-get-most-similar-documents\n",
    "word_tokens = \"happy year everyone family vacation familytime\".split(\" \")\n",
    "token_vector = model.infer_vector(word_tokens)\n",
    "sims = model.docvecs.most_similar([token_vector])\n",
    "print(sims)\n",
    "\n",
    "# print out the original text\n",
    "data = pd.read_csv(\"./datasets/10_retweet_text_doc2vec.csv\", sep='\\t')\n",
    "print('\\n', data.loc[14, 'retweet_text'], '\\n')\n",
    "print(data.loc[13, 'retweet_text'], '\\n')\n",
    "print(data.loc[240, 'retweet_text'], '\\n')\n",
    "print(data.loc[1, 'retweet_text'], '\\n\\n')\n",
    "\n",
    "\n",
    "word_tokens = \"4 years barack obama refused chinas workers paid price\".split(\" \")\n",
    "token_vector = model.infer_vector(word_tokens)\n",
    "sims = model.docvecs.most_similar([token_vector])\n",
    "print(sims)\n",
    "\n",
    "# print out the original text\n",
    "data = pd.read_csv(\"./datasets/10_retweet_text_doc2vec.csv\", sep='\\t')\n",
    "print('\\n', data.loc[440, 'retweet_text'], '\\n')\n",
    "print(data.loc[559, 'retweet_text'], '\\n')\n",
    "print(data.loc[576, 'retweet_text'], '\\n')\n",
    "print(data.loc[445, 'retweet_text'], '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "normal",
   "language": "python",
   "name": "normal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
