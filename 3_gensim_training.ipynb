{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "* change to space splitted basis.\n",
    "* remove duplicate rows and rows that consist too few element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Gensim Models - Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from multiprocessing import cpu_count\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train word2vec model by genism, with data: text8\n",
    "dataset = api.load('text8')\n",
    "data = [d for d in dataset]\n",
    "\n",
    "# train word2vec model. Default result vector size = 100\n",
    "model = Word2Vec(data, min_count=0, workers=cpu_count())\n",
    "\n",
    "model.save('./gensim_models/word2vec_model_text8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update our model with tweet dataset\n",
    "\n",
    "# create copora dictionary\n",
    "tweet_data = pd.read_csv('./datasets/tweet_stopwords_duplicates_short_removed_space_splitted.csv')\n",
    "data_list = tweet_data.iloc[:, 0].tolist()\n",
    "# tokenize data into words\n",
    "texts = [[text for text in tweet.split()] for tweet in data_list]\n",
    "# create dictionary by copora\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(type(dictionary))\n",
    "print(type(texts))\n",
    "\n",
    "\n",
    "# load text8 word2vec dataset\n",
    "model = Word2Vec.load('./gensim_models/word2vec_model_text8')\n",
    "print(model['media'], '\\n')\n",
    "\n",
    "# update the model with tweets dataset\n",
    "model.build_vocab(texts, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.iter)\n",
    "print(model['media'])\n",
    "\n",
    "# save the model\n",
    "model.save('./gensim_models/word2vec_model_text8_tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a word2vec model with tweet dataset only\n",
    "\n",
    "# create copora dictionary\n",
    "tweet_data = pd.read_csv('./datasets/tweet_stopwords_duplicates_short_removed_space_splitted.csv')\n",
    "data_list = tweet_data.iloc[:, 0].tolist()\n",
    "# tokenize data into words\n",
    "texts = [[text for text in tweet.split()] for tweet in data_list]\n",
    "# create dictionary by copora\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(type(dictionary))\n",
    "print(type(texts))\n",
    "\n",
    "# train word2vec model. Default result vector size = 100\n",
    "model = Word2Vec(texts, min_count=0, workers=cpu_count())\n",
    "print(model['trump'], '\\n')\n",
    "print(model['taiwan'])\n",
    "\n",
    "model.save('./gensim_models/word2vec_model_tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the result with only tweets data\n",
    "print(model.wv.most_similar('trump'), '\\n')\n",
    "print(model.wv.most_similar('taiwan'), '\\n')\n",
    "print(model.wv.most_similar('obama'), '\\n')\n",
    "print(model.wv.most_similar('freak'), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim Training - Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create 'tagged document'\n",
    "def create_tagged_document(list_of_list_of_word):\n",
    "    for i, list_of_word in enumerate(list_of_list_of_word):\n",
    "        yield gensim.models.doc2vec.TaggedDocument(list_of_word, [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 'tagged document' dataset of text8\n",
    "# load text8 dataset\n",
    "dataset = api.load('text8')\n",
    "data = [d for d in dataset]\n",
    "\n",
    "# create tagged document of text8\n",
    "training_data_text8 = list(create_tagged_document(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize doc2vec model\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=100)\n",
    "\n",
    "# build the vocabulary\n",
    "model.build_vocab(training_data_text8)\n",
    "\n",
    "# train the Doc2Vec model\n",
    "model.train(training_data_text8, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "model.save('./gensim_models/doc2vec_model_text8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset with text8 and tweets data\n",
    "tweet_data = pd.read_csv('./datasets/tweet_stopwords_duplicates_short_removed_space_splitted.csv')\n",
    "data_list = tweet_data.iloc[:, 0].tolist()\n",
    "\n",
    "# tokenize data into words\n",
    "training_tweet = [[text for text in tweet.split()] for tweet in data_list]\n",
    "\n",
    "# create tagged document with text8 and tweet\n",
    "training_data_text8_tweet = list(create_tagged_document(data + training_tweet))\n",
    "\n",
    "# create tagged document with tweet only\n",
    "training_data_tweet = list(create_tagged_document(training_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model for both text8 and tweets data\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=100)\n",
    "\n",
    "# build the vocabulary\n",
    "model.build_vocab(training_data_text8_tweet)\n",
    "\n",
    "# train the model\n",
    "model.train(training_data_text8_tweet, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "model.save('./gensim_models/doc2vec_model_text8_tweet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Tagged dataset and gensim models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create 'tagged document'\n",
    "def create_tagged_document(list_of_list_of_word):\n",
    "    for i, list_of_word in enumerate(list_of_list_of_word):\n",
    "        yield gensim.models.doc2vec.TaggedDocument(list_of_word, [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30567\n",
      "Index(['is_RT', 'retweet_from', 'whole_tweet_text', 'original_text',\n",
      "       'retweet_text', 'stopwords_removed_original_text'],\n",
      "      dtype='object')\n",
      "614\n",
      "Index(['is_RT', 'retweet_from', 'whole_tweet_text', 'original_text',\n",
      "       'retweet_text', 'stopwords_removed_retweet_text'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# prepare data by concating original and removed stopwords together\n",
    "original = pd.read_csv(\"./datasets/8_original_text_only_stopwords_removed.csv\", sep='\\t')\n",
    "retweet = pd.read_csv(\"./datasets/7_retweet_text_only_stopwords_removed.csv\", sep='\\t')\n",
    "\n",
    "print(len(original))\n",
    "print(original.columns)\n",
    "print(len(retweet))\n",
    "print(retweet.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize data into words\n",
    "training_tweet = [[text for text in str(tweet).split()] for tweet in retweet.loc[:, 'stopwords_removed_retweet_text']]\n",
    "# training_tweet = [[text for text in str(tweet).split()] for tweet in original.loc[:, 'stopwords_removed_original_text']]\n",
    "\n",
    "# create tagged document with tweet only\n",
    "training_data_tweet = list(create_tagged_document(training_tweet))\n",
    "\n",
    "# create a model for only tweets\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=100)\n",
    "\n",
    "# build the vocabulary\n",
    "model.build_vocab(training_data_tweet)\n",
    "\n",
    "# train the model\n",
    "model.train(training_data_tweet, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "model.save('./gensim_models/doc2vec_model_retweet')\n",
    "# model.save('./gensim_models/doc2vec_model_original')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset with Doc2Vec Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 614/614 [00:11<00:00, 55.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   is_RT     retweet_from                                   whole_tweet_text  \\\n",
      "0      1    SteveSGoddard  RT @SteveSGoddard: The @nytimes was thrilled b...   \n",
      "1      1   DonaldJTrumpJr  RT @DonaldJTrumpJr: Happy new year everyone. #...   \n",
      "2      1  VoteHillary2016  RT @VoteHillary2016: Trump says Taiwain leader...   \n",
      "3      1   NeonKnight1337  RT @NeonKnight1337: Donald Trump's call with T...   \n",
      "4      1    gatewaypundit  RT @gatewaypundit: The Trump Hotel Waikiki loo...   \n",
      "\n",
      "  original_text                                       retweet_text  \\\n",
      "0           NaN   The @nytimes was thrilled by Obama talking to...   \n",
      "1           NaN   Happy new year everyone. #newyear #family #va...   \n",
      "2           NaN   Trump says Taiwain leader called him, though ...   \n",
      "3           NaN   Donald Trump's call with Taiwan president was...   \n",
      "4           NaN   The Trump Hotel Waikiki looks like a lovely r...   \n",
      "\n",
      "                      stopwords_removed_retweet_text  feature_0  feature_1  \\\n",
      "0  nytimes thrilled Obama Castro horrified electe...  -0.068860   0.022312   \n",
      "1     Happy year everyone family vacation familytime  -0.091367   0.029216   \n",
      "2        Trump says Taiwain called him Trump staffer  -0.081161   0.003926   \n",
      "3               Donald call Taiwan surprise Official  -0.062142   0.077827   \n",
      "4  Trump Hotel Waikiki looks like resort realDona...  -0.025221   0.014050   \n",
      "\n",
      "   feature_2  feature_3  ...  feature_40  feature_41  feature_42  feature_43  \\\n",
      "0   0.099238  -0.032462  ...   -0.000563   -0.024736    0.152163   -0.124993   \n",
      "1   0.046879  -0.073239  ...    0.039415    0.073723    0.033815   -0.103018   \n",
      "2   0.062694   0.093167  ...    0.027239    0.114722    0.037387   -0.069017   \n",
      "3   0.044405  -0.031900  ...    0.051673   -0.042531    0.105240   -0.031540   \n",
      "4   0.046958  -0.052911  ...   -0.024418    0.096701    0.254161    0.004634   \n",
      "\n",
      "   feature_44  feature_45  feature_46  feature_47  feature_48  feature_49  \n",
      "0    0.213610   -0.190686   -0.024162   -0.311535   -0.050076    0.072496  \n",
      "1    0.114219   -0.161529   -0.080637   -0.209557    0.045275    0.074884  \n",
      "2    0.095425   -0.123071    0.057916   -0.208010   -0.010152    0.183908  \n",
      "3    0.113320   -0.059828    0.042292   -0.164491   -0.046977    0.007275  \n",
      "4    0.119699   -0.143366    0.031873   -0.313572    0.069602    0.029239  \n",
      "\n",
      "[5 rows x 56 columns]\n",
      "       is_RT   feature_0   feature_1   feature_2   feature_3   feature_4  \\\n",
      "count  614.0  614.000000  614.000000  614.000000  614.000000  614.000000   \n",
      "mean     1.0   -0.073265    0.038546    0.080521   -0.038584    0.107018   \n",
      "std      0.0    0.065125    0.104044    0.085776    0.093455    0.093431   \n",
      "min      1.0   -0.462662   -0.391170   -0.259107   -0.417089   -0.257266   \n",
      "25%      1.0   -0.100865   -0.004267    0.040577   -0.079652    0.058156   \n",
      "50%      1.0   -0.073313    0.036136    0.083850   -0.043088    0.112869   \n",
      "75%      1.0   -0.041468    0.067428    0.117132   -0.002987    0.148353   \n",
      "max      1.0    0.133836    0.896632    0.434219    0.390592    0.756288   \n",
      "\n",
      "        feature_5   feature_6   feature_7   feature_8  ...  feature_40  \\\n",
      "count  614.000000  614.000000  614.000000  614.000000  ...  614.000000   \n",
      "mean     0.141249   -0.023480   -0.138793    0.024779  ...    0.015368   \n",
      "std      0.089354    0.146002    0.116424    0.066271  ...    0.082896   \n",
      "min     -0.156907   -0.428035   -0.561793   -0.474605  ...   -0.389118   \n",
      "25%      0.095596   -0.082226   -0.195809   -0.000186  ...   -0.016549   \n",
      "50%      0.144328   -0.028252   -0.152148    0.024881  ...    0.008934   \n",
      "75%      0.185220    0.006964   -0.084731    0.049547  ...    0.040927   \n",
      "max      0.680526    0.940674    0.507686    0.355031  ...    0.574133   \n",
      "\n",
      "       feature_41  feature_42  feature_43  feature_44  feature_45  feature_46  \\\n",
      "count  614.000000  614.000000  614.000000  614.000000  614.000000  614.000000   \n",
      "mean    -0.016635    0.132407   -0.078290    0.150812   -0.160408    0.008619   \n",
      "std      0.088433    0.125110    0.131795    0.122056    0.134201    0.117508   \n",
      "min     -0.387776   -0.653255   -0.845010   -0.191995   -0.778021   -0.384144   \n",
      "25%     -0.044692    0.088848   -0.116913    0.089864   -0.227589   -0.033045   \n",
      "50%     -0.011779    0.140105   -0.076622    0.148179   -0.179172    0.005180   \n",
      "75%      0.011882    0.179210   -0.030603    0.193622   -0.090483    0.033111   \n",
      "max      0.594397    0.702730    0.545273    1.175299    0.627099    0.812787   \n",
      "\n",
      "       feature_47  feature_48  feature_49  \n",
      "count  614.000000  614.000000  614.000000  \n",
      "mean    -0.247288   -0.032217    0.061030  \n",
      "std      0.100469    0.089391    0.167509  \n",
      "min     -0.743698   -0.592997   -0.430799  \n",
      "25%     -0.299246   -0.062877   -0.003387  \n",
      "50%     -0.249460   -0.021183    0.060484  \n",
      "75%     -0.198352    0.008618    0.106689  \n",
      "max      0.022272    0.230779    1.334570  \n",
      "\n",
      "[8 rows x 51 columns]\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec.load('./gensim_models/doc2vec_model_retweet')\n",
    "\n",
    "for i, j in enumerate(tqdm(retweet.loc[:, 'stopwords_removed_retweet_text'])):\n",
    "    element_splitted = str(j).split(\" \")\n",
    "    feature_array = model.infer_vector(element_splitted)\n",
    "    for a, b in enumerate(feature_array):\n",
    "        column_name = 'feature_' + str(a)\n",
    "        retweet.loc[i, column_name] = b\n",
    "\n",
    "print(retweet.head())\n",
    "print(retweet.describe())\n",
    "retweet.to_csv('./datasets/10_retweet_text_doc2vec.csv', sep='\\t', header=True, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30567/30567 [14:47<00:00, 34.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   is_RT  retweet_from                                   whole_tweet_text  \\\n",
      "0      0           NaN  \"@MysticWolf12001: @realDonaldTrump C'mon, you...   \n",
      "1      0           NaN  \"@Chad_Williams91: @realDonaldTrump if you're ...   \n",
      "2      0           NaN  \"@HunterBalthazor: @realDonaldTrump if you ran...   \n",
      "3      0           NaN  \"@kyleraccio: @realDonaldTrump @Vinny_Titone I...   \n",
      "4      0           NaN                @HarryCraig96 @TrumpTowerNY Thanks!   \n",
      "\n",
      "                                       original_text  retweet_text  \\\n",
      "0  \"@MysticWolf12001: @realDonaldTrump C'mon, you...           NaN   \n",
      "1  \"@Chad_Williams91: @realDonaldTrump if you're ...           NaN   \n",
      "2  \"@HunterBalthazor: @realDonaldTrump if you ran...           NaN   \n",
      "3  \"@kyleraccio: @realDonaldTrump @Vinny_Titone I...           NaN   \n",
      "4                @HarryCraig96 @TrumpTowerNY Thanks!           NaN   \n",
      "\n",
      "                     stopwords_removed_original_text  feature_0  feature_1  \\\n",
      "0           MysticWolf12001 Cmon one beating Clinton  -0.472350   0.265750   \n",
      "1  ChadWilliams91 Ill move states you are hired T...   0.285447   0.163334   \n",
      "2      HunterBalthazor ran president vote back track  -0.087933   0.054794   \n",
      "3  kyleraccio VinnyTitone think hell lead polls l...   1.615295  -0.307720   \n",
      "4                                       HarryCraig96  -0.002076   0.006711   \n",
      "\n",
      "   feature_2  feature_3  ...  feature_40  feature_41  feature_42  feature_43  \\\n",
      "0  -0.075877   0.105779  ...    0.143317    0.124259    0.200973   -0.552289   \n",
      "1  -0.093235   0.382448  ...   -0.298301    0.355428    1.187480   -0.849795   \n",
      "2   1.397250  -0.333880  ...   -0.051860    1.087389    0.081745   -0.632128   \n",
      "3   1.498087   0.333555  ...   -0.462321    0.558051   -0.957069   -0.365242   \n",
      "4  -0.006008  -0.005556  ...   -0.001178   -0.002132   -0.000096    0.003575   \n",
      "\n",
      "   feature_44  feature_45  feature_46  feature_47  feature_48  feature_49  \n",
      "0   -0.210194   -0.188726    0.121643   -0.246263    0.264947    0.085403  \n",
      "1    0.043440   -1.361078    0.328783    0.312819   -0.558430    0.933321  \n",
      "2   -0.662427   -0.726879   -0.265769   -0.203443   -0.867413   -0.258126  \n",
      "3   -0.660991   -1.461468    0.329077   -1.691229   -0.323408    0.593998  \n",
      "4    0.002076   -0.005620   -0.002212   -0.004653   -0.008833    0.007735  \n",
      "\n",
      "[5 rows x 56 columns]\n",
      "         is_RT  retweet_from  retweet_text     feature_0     feature_1  \\\n",
      "count  30567.0           0.0           0.0  30567.000000  30567.000000   \n",
      "mean       0.0           NaN           NaN      0.102301      0.101558   \n",
      "std        0.0           NaN           NaN      0.491129      0.502098   \n",
      "min        0.0           NaN           NaN     -3.014651     -3.275907   \n",
      "25%        0.0           NaN           NaN     -0.151609     -0.164497   \n",
      "50%        0.0           NaN           NaN      0.101898      0.107242   \n",
      "75%        0.0           NaN           NaN      0.367397      0.378024   \n",
      "max        0.0           NaN           NaN      2.564242      2.621286   \n",
      "\n",
      "          feature_2     feature_3     feature_4     feature_5     feature_6  \\\n",
      "count  30567.000000  30567.000000  30567.000000  30567.000000  30567.000000   \n",
      "mean       0.365121     -0.178124      0.149506      0.047234      0.019007   \n",
      "std        0.500056      0.449404      0.465165      0.515179      0.487768   \n",
      "min       -2.859653     -2.968643     -2.613342     -2.795368     -2.855796   \n",
      "25%        0.090774     -0.426163     -0.093159     -0.236254     -0.239798   \n",
      "50%        0.367883     -0.164806      0.150276      0.043979      0.017134   \n",
      "75%        0.647217      0.066230      0.404381      0.327466      0.283026   \n",
      "max        3.097231      2.132897      2.220628      2.978934      3.727455   \n",
      "\n",
      "       ...    feature_40    feature_41    feature_42    feature_43  \\\n",
      "count  ...  30567.000000  30567.000000  30567.000000  30567.000000   \n",
      "mean   ...      0.076604     -0.021165     -0.037229     -0.284420   \n",
      "std    ...      0.483204      0.500617      0.454456      0.456667   \n",
      "min    ...     -2.792202     -2.947626     -2.684136     -2.863769   \n",
      "25%    ...     -0.175770     -0.285594     -0.276756     -0.540528   \n",
      "50%    ...      0.081296     -0.026458     -0.036268     -0.277149   \n",
      "75%    ...      0.339991      0.249313      0.212773     -0.023867   \n",
      "max    ...      2.682476      2.703617      2.658982      1.907356   \n",
      "\n",
      "         feature_44    feature_45    feature_46    feature_47    feature_48  \\\n",
      "count  30567.000000  30567.000000  30567.000000  30567.000000  30567.000000   \n",
      "mean      -0.279093     -0.178741      0.022463     -0.224311      0.024828   \n",
      "std        0.514109      0.528784      0.497779      0.475216      0.509150   \n",
      "min       -3.093233     -3.406670     -2.665883     -3.177602     -2.980092   \n",
      "25%       -0.546260     -0.461177     -0.247218     -0.486751     -0.239814   \n",
      "50%       -0.269558     -0.180288      0.028333     -0.213971      0.036310   \n",
      "75%        0.000698      0.112183      0.296656      0.036573      0.297696   \n",
      "max        3.295016      3.276173      2.437760      2.408928      3.247182   \n",
      "\n",
      "         feature_49  \n",
      "count  30567.000000  \n",
      "mean       0.009335  \n",
      "std        0.507971  \n",
      "min       -2.965598  \n",
      "25%       -0.273044  \n",
      "50%        0.003421  \n",
      "75%        0.280803  \n",
      "max        3.009715  \n",
      "\n",
      "[8 rows x 53 columns]\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec.load('./gensim_models/doc2vec_model_original')\n",
    "\n",
    "for i, j in enumerate(tqdm(original.loc[:, 'stopwords_removed_original_text'])):\n",
    "    element_splitted = str(j).split(\" \")\n",
    "    feature_array = model.infer_vector(element_splitted)\n",
    "    for a, b in enumerate(feature_array):\n",
    "        column_name = 'feature_' + str(a)\n",
    "        original.loc[i, column_name] = b\n",
    "\n",
    "print(original.head())\n",
    "print(original.describe())\n",
    "original.to_csv('./datasets/10_original_text_doc2vec.csv', sep='\\t', header=True, index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "normal",
   "language": "python",
   "name": "normal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
