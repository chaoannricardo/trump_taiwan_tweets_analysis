{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "* change to space splitted basis.\n",
    "* remove duplicate rows and rows that consist too few element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Gensim Models - Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from multiprocessing import cpu_count\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train word2vec model by genism, with data: text8\n",
    "dataset = api.load('text8')\n",
    "data = [d for d in dataset]\n",
    "\n",
    "# train word2vec model. Default result vector size = 100\n",
    "model = Word2Vec(data, min_count=0, workers=cpu_count())\n",
    "\n",
    "model.save('./gensim_models/word2vec_model_text8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update our model with tweet dataset\n",
    "\n",
    "# create copora dictionary\n",
    "tweet_data = pd.read_csv('./datasets/tweet_stopwords_duplicates_short_removed_space_splitted.csv')\n",
    "data_list = tweet_data.iloc[:, 0].tolist()\n",
    "# tokenize data into words\n",
    "texts = [[text for text in tweet.split()] for tweet in data_list]\n",
    "# create dictionary by copora\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(type(dictionary))\n",
    "print(type(texts))\n",
    "\n",
    "\n",
    "# load text8 word2vec dataset\n",
    "model = Word2Vec.load('./gensim_models/word2vec_model_text8')\n",
    "print(model['media'], '\\n')\n",
    "\n",
    "# update the model with tweets dataset\n",
    "model.build_vocab(texts, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.iter)\n",
    "print(model['media'])\n",
    "\n",
    "# save the model\n",
    "model.save('./gensim_models/word2vec_model_text8_tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a word2vec model with tweet dataset only\n",
    "\n",
    "# create copora dictionary\n",
    "tweet_data = pd.read_csv('./datasets/tweet_stopwords_duplicates_short_removed_space_splitted.csv')\n",
    "data_list = tweet_data.iloc[:, 0].tolist()\n",
    "# tokenize data into words\n",
    "texts = [[text for text in tweet.split()] for tweet in data_list]\n",
    "# create dictionary by copora\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(type(dictionary))\n",
    "print(type(texts))\n",
    "\n",
    "# train word2vec model. Default result vector size = 100\n",
    "model = Word2Vec(texts, min_count=0, workers=cpu_count())\n",
    "print(model['trump'], '\\n')\n",
    "print(model['taiwan'])\n",
    "\n",
    "model.save('./gensim_models/word2vec_model_tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the result with only tweets data\n",
    "print(model.wv.most_similar('trump'), '\\n')\n",
    "print(model.wv.most_similar('taiwan'), '\\n')\n",
    "print(model.wv.most_similar('obama'), '\\n')\n",
    "print(model.wv.most_similar('freak'), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim Training - Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create 'tagged document'\n",
    "def create_tagged_document(list_of_list_of_word):\n",
    "    for i, list_of_word in enumerate(list_of_list_of_word):\n",
    "        yield gensim.models.doc2vec.TaggedDocument(list_of_word, [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 'tagged document' dataset of text8\n",
    "# load text8 dataset\n",
    "dataset = api.load('text8')\n",
    "data = [d for d in dataset]\n",
    "\n",
    "# create tagged document of text8\n",
    "training_data_text8 = list(create_tagged_document(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize doc2vec model\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=100)\n",
    "\n",
    "# build the vocabulary\n",
    "model.build_vocab(training_data_text8)\n",
    "\n",
    "# train the Doc2Vec model\n",
    "model.train(training_data_text8, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "model.save('./gensim_models/doc2vec_model_text8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset with text8 and tweets data\n",
    "tweet_data = pd.read_csv('./datasets/tweet_stopwords_duplicates_short_removed_space_splitted.csv')\n",
    "data_list = tweet_data.iloc[:, 0].tolist()\n",
    "\n",
    "# tokenize data into words\n",
    "training_tweet = [[text for text in tweet.split()] for tweet in data_list]\n",
    "\n",
    "# create tagged document with text8 and tweet\n",
    "training_data_text8_tweet = list(create_tagged_document(data + training_tweet))\n",
    "\n",
    "# create tagged document with tweet only\n",
    "training_data_tweet = list(create_tagged_document(training_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model for both text8 and tweets data\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=100)\n",
    "\n",
    "# build the vocabulary\n",
    "model.build_vocab(training_data_text8_tweet)\n",
    "\n",
    "# train the model\n",
    "model.train(training_data_text8_tweet, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "model.save('./gensim_models/doc2vec_model_text8_tweet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Tagged dataset and gensim models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create 'tagged document'\n",
    "def create_tagged_document(list_of_list_of_word):\n",
    "    for i, list_of_word in enumerate(list_of_list_of_word):\n",
    "        yield gensim.models.doc2vec.TaggedDocument(list_of_word, [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30567\n",
      "Index(['is_RT', 'retweet_from', 'whole_tweet_text', 'original_text',\n",
      "       'retweet_text', 'cleaned_original_text'],\n",
      "      dtype='object')\n",
      "614\n",
      "Index(['is_RT', 'retweet_from', 'whole_tweet_text', 'original_text',\n",
      "       'retweet_text', 'cleaned_retweet_text'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# prepare data by concating original and removed stopwords together\n",
    "original = pd.read_csv(\"./datasets/12_original_text_only_cleaned.csv\", sep='\\t')\n",
    "retweet = pd.read_csv(\"./datasets/11_retweet_text_only_cleaned.csv\", sep='\\t')\n",
    "\n",
    "print(len(original))\n",
    "print(original.columns)\n",
    "print(len(retweet))\n",
    "print(retweet.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize data into words\n",
    "training_tweet = [[text for text in str(tweet).split()] for tweet in retweet.loc[:, 'cleaned_retweet_text']]\n",
    "# training_tweet = [[text for text in str(tweet).split()] for tweet in original.loc[:, 'cleaned_original_text']]\n",
    "\n",
    "# create tagged document with tweet only\n",
    "training_data_tweet = list(create_tagged_document(training_tweet))\n",
    "\n",
    "# create a model for only tweets\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=100)\n",
    "\n",
    "# build the vocabulary\n",
    "model.build_vocab(training_data_tweet)\n",
    "\n",
    "# train the model\n",
    "model.train(training_data_tweet, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "model.save('./gensim_models/doc2vec_model_retweet_new')\n",
    "# model.save('./gensim_models/doc2vec_model_original_new')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset with Doc2Vec Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 614/614 [00:12<00:00, 48.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   is_RT     retweet_from                                   whole_tweet_text  \\\n",
      "0      1    SteveSGoddard  RT @SteveSGoddard: The @nytimes was thrilled b...   \n",
      "1      1   DonaldJTrumpJr  RT @DonaldJTrumpJr: Happy new year everyone. #...   \n",
      "2      1  VoteHillary2016  RT @VoteHillary2016: Trump says Taiwain leader...   \n",
      "3      1   NeonKnight1337  RT @NeonKnight1337: Donald Trump's call with T...   \n",
      "4      1    gatewaypundit  RT @gatewaypundit: The Trump Hotel Waikiki loo...   \n",
      "\n",
      "  original_text                                       retweet_text  \\\n",
      "0           NaN   The @nytimes was thrilled by Obama talking to...   \n",
      "1           NaN   Happy new year everyone. #newyear #family #va...   \n",
      "2           NaN   Trump says Taiwain leader called him, though ...   \n",
      "3           NaN   Donald Trump's call with Taiwan president was...   \n",
      "4           NaN   The Trump Hotel Waikiki looks like a lovely r...   \n",
      "\n",
      "                                cleaned_retweet_text  feature_0  feature_1  \\\n",
      "0  the nytimes was thrilled by obama talking to c...   0.351525  -0.565295   \n",
      "1  happy new year everyone newyear family vacatio...   0.177054  -0.084449   \n",
      "2  trump says taiwain leader called him though ta...   0.214278  -0.127029   \n",
      "3  donald trumps call with taiwan president was n...   0.359044  -0.589259   \n",
      "4  the trump hotel waikiki looks like a lovely re...  -0.060776  -0.299661   \n",
      "\n",
      "   feature_2  feature_3  ...  feature_40  feature_41  feature_42  feature_43  \\\n",
      "0   0.817368   0.127292  ...    0.212919   -0.304902    0.704935   -1.088349   \n",
      "1  -0.703282   0.431147  ...   -0.526049    0.124294    0.055487    0.411121   \n",
      "2   0.033569   1.101794  ...    0.129149   -0.279915   -0.317819   -1.359353   \n",
      "3   0.278139  -0.197522  ...   -0.279575   -0.041932   -0.357189   -0.008904   \n",
      "4  -0.253958  -0.011773  ...    0.087844   -0.089346   -0.341705    0.050415   \n",
      "\n",
      "   feature_44  feature_45  feature_46  feature_47  feature_48  feature_49  \n",
      "0   -0.464934    1.011710    0.292147    0.068312   -0.733953    0.236715  \n",
      "1    0.495081   -0.043996   -0.473274    0.736473   -0.326077    0.171160  \n",
      "2   -0.338792    0.475849   -0.489580    0.528342   -0.310211   -0.668882  \n",
      "3   -0.376021    0.176018   -0.103770    0.143358   -0.074610    0.052973  \n",
      "4   -0.013642    0.129445    0.348212    0.174998   -0.136396    0.124857  \n",
      "\n",
      "[5 rows x 56 columns]\n",
      "       is_RT   feature_0   feature_1   feature_2   feature_3   feature_4  \\\n",
      "count  614.0  614.000000  614.000000  614.000000  614.000000  614.000000   \n",
      "mean     1.0    0.118057   -0.183253   -0.110804   -0.031325   -0.366903   \n",
      "std      0.0    0.339650    0.333804    0.510741    0.485501    0.368898   \n",
      "min      1.0   -1.096507   -1.645107   -1.862871   -1.843743   -1.811498   \n",
      "25%      1.0   -0.088156   -0.364365   -0.426888   -0.349984   -0.604710   \n",
      "50%      1.0    0.110326   -0.141353   -0.127101   -0.020406   -0.352361   \n",
      "75%      1.0    0.283472    0.009309    0.198295    0.253822   -0.120667   \n",
      "max      1.0    1.704202    1.070674    1.928964    1.418595    0.699223   \n",
      "\n",
      "        feature_5   feature_6   feature_7   feature_8  ...  feature_40  \\\n",
      "count  614.000000  614.000000  614.000000  614.000000  ...  614.000000   \n",
      "mean     0.118707   -0.106824    0.039484   -0.021816  ...   -0.161703   \n",
      "std      0.501141    0.395887    0.348938    0.414348  ...    0.383317   \n",
      "min     -1.428092   -1.414939   -0.970554   -1.339696  ...   -1.448875   \n",
      "25%     -0.190186   -0.318202   -0.138301   -0.291610  ...   -0.395550   \n",
      "50%      0.075210   -0.085792    0.049086   -0.022536  ...   -0.167467   \n",
      "75%      0.402856    0.096180    0.235724    0.230226  ...    0.046986   \n",
      "max      2.299089    1.064910    1.138146    1.331520  ...    1.148846   \n",
      "\n",
      "       feature_41  feature_42  feature_43  feature_44  feature_45  feature_46  \\\n",
      "count  614.000000  614.000000  614.000000  614.000000  614.000000  614.000000   \n",
      "mean     0.040312   -0.113148    0.037194   -0.060170    0.229812   -0.033655   \n",
      "std      0.303464    0.455606    0.430256    0.395617    0.480831    0.398954   \n",
      "min     -1.198351   -2.042323   -1.728813   -1.631720   -1.364490   -1.420308   \n",
      "25%     -0.132121   -0.359589   -0.184029   -0.297846   -0.056056   -0.249904   \n",
      "50%      0.035097   -0.104371    0.056753   -0.038914    0.185386   -0.053767   \n",
      "75%      0.220213    0.152797    0.321661    0.212533    0.492799    0.188561   \n",
      "max      1.020551    1.220980    1.284695    1.157474    2.010747    1.444923   \n",
      "\n",
      "       feature_47  feature_48  feature_49  \n",
      "count  614.000000  614.000000  614.000000  \n",
      "mean     0.064660   -0.071153    0.088782  \n",
      "std      0.328073    0.399552    0.460632  \n",
      "min     -1.014173   -1.673254   -1.208564  \n",
      "25%     -0.135589   -0.310135   -0.199585  \n",
      "50%      0.078182   -0.055959    0.069099  \n",
      "75%      0.259506    0.162479    0.359234  \n",
      "max      1.582329    1.467533    1.947114  \n",
      "\n",
      "[8 rows x 51 columns]\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec.load('./gensim_models/doc2vec_model_retweet_new')\n",
    "\n",
    "for i, j in enumerate(tqdm(retweet.loc[:, 'cleaned_retweet_text'])):\n",
    "    element_splitted = str(j).split(\" \")\n",
    "    feature_array = model.infer_vector(element_splitted)\n",
    "    for a, b in enumerate(feature_array):\n",
    "        column_name = 'feature_' + str(a)\n",
    "        retweet.loc[i, column_name] = b\n",
    "\n",
    "print(retweet.head())\n",
    "print(retweet.describe())\n",
    "retweet.to_csv('./datasets/13_retweet_text_doc2vec_new.csv', sep='\\t', header=True, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30567/30567 [16:37<00:00, 30.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   is_RT  retweet_from                                   whole_tweet_text  \\\n",
      "0      0           NaN  \"@MysticWolf12001: @realDonaldTrump C'mon, you...   \n",
      "1      0           NaN  \"@Chad_Williams91: @realDonaldTrump if you're ...   \n",
      "2      0           NaN  \"@HunterBalthazor: @realDonaldTrump if you ran...   \n",
      "3      0           NaN  \"@kyleraccio: @realDonaldTrump @Vinny_Titone I...   \n",
      "4      0           NaN                @HarryCraig96 @TrumpTowerNY Thanks!   \n",
      "\n",
      "                                       original_text  retweet_text  \\\n",
      "0  \"@MysticWolf12001: @realDonaldTrump C'mon, you...           NaN   \n",
      "1  \"@Chad_Williams91: @realDonaldTrump if you're ...           NaN   \n",
      "2  \"@HunterBalthazor: @realDonaldTrump if you ran...           NaN   \n",
      "3  \"@kyleraccio: @realDonaldTrump @Vinny_Titone I...           NaN   \n",
      "4                @HarryCraig96 @TrumpTowerNY Thanks!           NaN   \n",
      "\n",
      "                               cleaned_original_text  feature_0  feature_1  \\\n",
      "0  mysticwolf12001 cmon you know youre the only o...  -0.768592  -0.792329   \n",
      "1  chadwilliams91 if youre president ill move to ...   0.921706  -1.044010   \n",
      "2  hunterbalthazor if you ran for president you h...   0.593363  -0.937204   \n",
      "3  kyleraccio vinnytitone i think hell lead the p...  -1.371773  -1.320379   \n",
      "4                                harrycraig96 thanks   0.238004  -0.328827   \n",
      "\n",
      "   feature_2  feature_3  ...  feature_40  feature_41  feature_42  feature_43  \\\n",
      "0   0.585194   1.035691  ...    0.314116   -0.414782    0.424944   -1.626707   \n",
      "1   0.845550  -0.481496  ...    1.065640    0.169806    1.559093    2.022515   \n",
      "2   0.891155   1.787635  ...    0.130576    0.632830    0.901615   -0.486742   \n",
      "3   0.048346   0.998821  ...   -0.744296   -0.869827    0.967274    1.104119   \n",
      "4   0.389766   0.018991  ...   -0.054484   -0.065257    0.216145    0.133834   \n",
      "\n",
      "   feature_44  feature_45  feature_46  feature_47  feature_48  feature_49  \n",
      "0   -0.525767    1.040392   -0.418054   -0.899761    0.344677    1.182745  \n",
      "1   -0.505934    0.026736    0.732342    1.288271   -0.895938   -0.214861  \n",
      "2   -0.846205    0.267818    1.795387   -0.094963    0.367216    1.055712  \n",
      "3   -0.399383   -0.907037    0.254770    0.466751    0.778327   -0.327616  \n",
      "4   -0.328141    0.097133    0.276053    0.041791    0.260765    0.255579  \n",
      "\n",
      "[5 rows x 56 columns]\n",
      "         is_RT  retweet_from  retweet_text     feature_0     feature_1  \\\n",
      "count  30567.0           0.0           0.0  30567.000000  30567.000000   \n",
      "mean       0.0           NaN           NaN      0.096971     -0.360446   \n",
      "std        0.0           NaN           NaN      0.851648      0.819463   \n",
      "min        0.0           NaN           NaN     -4.410851     -4.473442   \n",
      "25%        0.0           NaN           NaN     -0.400830     -0.832553   \n",
      "50%        0.0           NaN           NaN      0.107386     -0.338314   \n",
      "75%        0.0           NaN           NaN      0.596454      0.109500   \n",
      "max        0.0           NaN           NaN      4.283805      4.115561   \n",
      "\n",
      "          feature_2     feature_3     feature_4     feature_5     feature_6  \\\n",
      "count  30567.000000  30567.000000  30567.000000  30567.000000  30567.000000   \n",
      "mean       0.825639      0.327620     -0.164400     -0.256674     -0.074276   \n",
      "std        0.750029      0.811215      0.800782      0.820821      0.765274   \n",
      "min       -2.934982     -3.245559     -3.754341     -5.112330     -3.951014   \n",
      "25%        0.370323     -0.144180     -0.643751     -0.721909     -0.518055   \n",
      "50%        0.802094      0.319101     -0.177981     -0.220302     -0.037991   \n",
      "75%        1.275490      0.817539      0.316055      0.216878      0.371302   \n",
      "max        4.921671      5.014572      3.601880      4.006581      3.252676   \n",
      "\n",
      "       ...    feature_40    feature_41    feature_42    feature_43  \\\n",
      "count  ...  30567.000000  30567.000000  30567.000000  30567.000000   \n",
      "mean   ...     -0.099039      0.154990      0.072364      0.171225   \n",
      "std    ...      0.844388      0.765289      0.853346      0.857211   \n",
      "min    ...     -4.620774     -3.494535     -3.938706     -3.635178   \n",
      "25%    ...     -0.588679     -0.289563     -0.418793     -0.320788   \n",
      "50%    ...     -0.079617      0.140078      0.080871      0.178432   \n",
      "75%    ...      0.387959      0.600978      0.568338      0.672341   \n",
      "max    ...      4.010258      3.867847      3.995659      4.061174   \n",
      "\n",
      "         feature_44    feature_45    feature_46    feature_47    feature_48  \\\n",
      "count  30567.000000  30567.000000  30567.000000  30567.000000  30567.000000   \n",
      "mean      -0.502098      0.216883      0.302121      0.412707      0.112738   \n",
      "std        0.752072      0.783794      0.847447      0.807634      0.740814   \n",
      "min       -4.478022     -3.367142     -4.344168     -4.039300     -3.424449   \n",
      "25%       -0.952051     -0.235231     -0.194121     -0.040858     -0.319612   \n",
      "50%       -0.491583      0.204179      0.292118      0.391820      0.122616   \n",
      "75%       -0.063820      0.667211      0.795380      0.895941      0.543887   \n",
      "max        3.020556      4.417666      5.283151      4.291478      3.685886   \n",
      "\n",
      "         feature_49  \n",
      "count  30567.000000  \n",
      "mean       0.488220  \n",
      "std        0.860491  \n",
      "min       -3.935231  \n",
      "25%        0.001423  \n",
      "50%        0.462709  \n",
      "75%        0.987901  \n",
      "max        4.510049  \n",
      "\n",
      "[8 rows x 53 columns]\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec.load('./gensim_models/doc2vec_model_original_new')\n",
    "\n",
    "for i, j in enumerate(tqdm(original.loc[:, 'cleaned_original_text'])):\n",
    "    element_splitted = str(j).split(\" \")\n",
    "    feature_array = model.infer_vector(element_splitted)\n",
    "    for a, b in enumerate(feature_array):\n",
    "        column_name = 'feature_' + str(a)\n",
    "        original.loc[i, column_name] = b\n",
    "\n",
    "print(original.head())\n",
    "print(original.describe())\n",
    "original.to_csv('./datasets/14_original_text_doc2vec_new.csv', sep='\\t', header=True, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim Vector Relations Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy\n",
      " [('birthday', 0.9392526149749756), ('hampshire', 0.9201045036315918), ('x1f1fax1f1f8', 0.8853327035903931), ('donaldjtrumpjr', 0.8516042232513428), ('simply', 0.8400300741195679), ('thank', 0.8388058543205261), ('peace', 0.8381062150001526), ('nevada', 0.8368520736694336), ('primary', 0.8350822925567627), ('incredible', 0.832456111907959)] \n",
      "\n",
      "nytimes\n",
      " [('dnc', 0.9539414644241333), ('foolish', 0.9511896371841431), ('taiwanese', 0.9431979656219482), ('interest', 0.9420510530471802), ('blasts', 0.9390931725502014), ('conflict', 0.9382774829864502), ('means', 0.9286386370658875), ('former', 0.9256524443626404), ('hysterical', 0.9247620105743408), ('arranged', 0.9226740598678589)] \n",
      "\n",
      "trump\n",
      " [('taiwan', 0.8802579045295715), ('arranged', 0.8785498738288879), ('taiwanese', 0.8737508058547974), ('spoke', 0.8711260557174683), ('msm', 0.8688294887542725), ('telephone', 0.8656235337257385), ('foolish', 0.8620258569717407), ('development', 0.8605897426605225), ('which', 0.8572181463241577), ('taking', 0.8571624159812927)] \n",
      "\n",
      "realdonaldtrump\n",
      " [('taiwan', 0.8802579045295715), ('arranged', 0.8785498738288879), ('taiwanese', 0.8737508058547974), ('spoke', 0.8711260557174683), ('msm', 0.8688294887542725), ('telephone', 0.8656235337257385), ('foolish', 0.8620258569717407), ('development', 0.8605897426605225), ('which', 0.8572181463241577), ('taking', 0.8571624159812927)] \n",
      "\n",
      "obama\n",
      " [('sells', 0.9508646726608276), ('billion', 0.9385319948196411), ('sold', 0.9181928038597107), ('weapons', 0.9126449823379517), ('183', 0.9081997871398926), ('arms', 0.9020462036132812), ('worth', 0.9002625942230225), ('500', 0.8975414037704468), ('18', 0.8894564509391785), ('iran', 0.8709158897399902)] \n",
      "\n",
      "taiwan\n",
      " [('call', 0.9013136625289917), ('conflict', 0.8991833925247192), ('conversation', 0.8942769765853882), ('trumps', 0.891249418258667), ('isnt', 0.8903273344039917), ('fiasco', 0.8893347978591919), ('development', 0.8880508542060852), ('trump', 0.8802579641342163), ('phone', 0.8768693804740906), ('goes', 0.8762312531471252)] \n",
      "\n",
      "china\n",
      " [('speaking', 0.9140967726707458), ('off', 0.9117621779441833), ('rift', 0.9077248573303223), ('complaint', 0.8984481692314148), ('major', 0.8983495235443115), ('pissing', 0.897532045841217), ('protest', 0.8975266814231873), ('lodges', 0.8935257196426392), ('diplomatic', 0.893192708492279), ('lodged', 0.8909904956817627)] \n",
      "\n",
      "beijing\n",
      " [('controversial', 0.9776982665061951), ('ap', 0.9565733075141907), ('yi', 0.9537160396575928), ('infuriate', 0.953281044960022), ('ingwen', 0.9498854875564575), ('history', 0.9488028287887573), ('sparks', 0.9461030960083008), ('tsai', 0.9458901882171631), ('between', 0.9412098526954651), ('hopes', 0.9408260583877563)] \n",
      "\n",
      "cnn\n",
      " [('hbo', 0.9669398665428162), ('foxandfriends', 0.8743792772293091), ('dallas', 0.8705385327339172), ('columbus', 0.8665788173675537), ('foxnews', 0.8587372303009033), ('tonight', 0.841569721698761), ('rally', 0.8404409885406494), ('danscavino', 0.8386516571044922), ('national', 0.8355013132095337), ('friend', 0.8308085203170776)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# checking words similarities\n",
    "\n",
    "model = gensim.models.doc2vec.Doc2Vec.load('./gensim_models/doc2vec_model_retweet_new')\n",
    "\n",
    "print('happy\\n', model.wv.most_similar('happy'), '\\n')\n",
    "print('nytimes\\n', model.wv.most_similar('nytimes'), '\\n')\n",
    "print('trump\\n', model.wv.most_similar('trump'), '\\n')\n",
    "print('realdonaldtrump\\n', model.wv.most_similar('trump'), '\\n')\n",
    "print('obama\\n', model.wv.most_similar('obama'), '\\n')\n",
    "print('taiwan\\n', model.wv.most_similar('taiwan'), '\\n')\n",
    "print('china\\n', model.wv.most_similar('china'), '\\n')\n",
    "print('beijing\\n', model.wv.most_similar('beijing'), '\\n')\n",
    "print('cnn\\n', model.wv.most_similar('cnn'), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.20089632 -0.43371838  0.40903682  0.5232539  -0.09847506  0.06498223\n",
      " -0.5256974   0.06251621 -0.67933595  0.61427474 -0.17155817  0.0666979\n",
      " -0.13634105  0.08373062 -0.42614767 -0.04508951 -0.51630783 -0.78576875\n",
      " -0.06757877  0.21523958  0.40046397  0.26988435 -0.2397853   0.38511437\n",
      "  0.01208746 -0.7255458  -0.6818082   0.4965771  -1.0080531  -0.9309631\n",
      "  0.08889045 -0.49097294  0.28477725  0.43024912 -0.70103633  0.41717553\n",
      " -0.54872406 -0.34307778 -0.951142    0.1078896   0.02034028 -0.5034523\n",
      " -0.68035924 -0.7232569  -0.545198    0.6891764  -0.54035807  0.32911068\n",
      "  0.23385102 -0.7544698 ] \n",
      "\n",
      "[-0.04689683 -0.6998818   0.96006435 -0.06521887 -0.19370526 -0.09437462\n",
      " -0.84830284 -0.32293    -1.3422617   0.5378286  -0.6157186   0.1898993\n",
      "  0.01309427  0.19617535 -0.31892866 -1.0963701  -1.3945589  -1.3643827\n",
      "  0.3050841   0.32039866  0.34118     0.58407784 -0.3048922   0.44602215\n",
      "  0.52530587 -1.2686615  -0.70920163  0.3370944  -1.2055398  -1.2726709\n",
      "  0.24273778 -0.42144412  0.55881476  0.59520465 -0.9136528   0.14208713\n",
      " -0.7947347  -0.72292745 -1.0930057  -0.4058047  -0.22125176 -0.14189887\n",
      " -0.5649136  -0.96228915 -1.1450272   0.9294831  -0.65305865 -0.19683869\n",
      "  0.17021245 -0.87900484] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print out vector of words in our dataset\n",
    "\n",
    "print(model['trump'], '\\n')\n",
    "print(model['taiwan'], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.8908886909484863), (183, 0.7266837954521179), (584, 0.6549695134162903), (118, 0.6189082264900208), (455, 0.6054126620292664), (398, 0.5983391404151917), (31, 0.5962131023406982), (530, 0.5778654217720032), (166, 0.5736516714096069), (113, 0.5693560838699341)]\n",
      "the nytimes was thrilled by obama talking to communist dictator castro and horrified by trump talking to the elected le\n",
      "\n",
      "  ONLY REASON it was \"protocol\" to not talk to Taiwan publicly is Obama is too cowardly &amp; TERRIFIED of offending China.??? \n",
      "\n",
      " Corrupt MSM says President-Elect Trump is not allowed to speak w/ Taiwan because Communist China says so. China does NOT tell??n \n",
      "\n",
      " Donald Trump fell for a ?ittle trick??performed by Taiwan, China says https://t.co/64eU35xFlIcede \n",
      "\n",
      " Story is not: Trump broke diplomatic norms by calling Taiwan. \n",
      "\n",
      "\n",
      "[(38, 0.8712615966796875), (40, 0.6251914501190186), (569, 0.6199179887771606), (324, 0.5989843606948853), (120, 0.5843089818954468), (316, 0.5767746567726135), (393, 0.5718825459480286), (494, 0.5670040845870972), (112, 0.544963002204895), (513, 0.5410667657852173)]\n",
      "\n",
      "  Incompetence is really the primary threat from the #Trump administration. https://t.co/tbhXqikV6i \n",
      "\n",
      " Live footage from the State Department in wake of Trump Taiwan call https://t.co/DWRx8vRDoj \n",
      "\n",
      " This extract from my book Easternisation gives a flavour of why Trump's Taiwan phone call is so potentially dangerous ht??n \n",
      "\n",
      " .@timkaine's Abortion Flip-Flops: From Valuing The Sanctity of Life --> Pro-Abortion Demagogue #VPdebate https://t.co/aK4061… \n",
      "\n",
      "\n",
      "[(128, 0.9065233469009399), (579, 0.8177934288978577), (134, 0.7931699752807617), (162, 0.7657575607299805), (96, 0.7611484527587891), (398, 0.7336968183517456), (468, 0.7309101819992065), (56, 0.7040701508522034), (445, 0.7023875713348389), (510, 0.6922720670700073)]\n",
      "\n",
      "  Obama turned his back on Taiwan \n",
      "\n",
      " Trump: 'Is the Boston Killer Eligible for Obama Care to Bring Him Back to Health?' http://t.co/7Z4JlS4Er5 \n",
      "\n",
      " Unlike Obama, Trump isn't going to bow down to foreign leaders or begin his presidency displaying signs of weakness. \n",
      "\n",
      " Obama? been so dignified about a peaceful transition of power?地nd Trump just stabbed him in the back by calling Taiwan??n??ti \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reference: https://stackoverflow.com/questions/42781292/doc2vec-get-most-similar-documents\n",
    "word_tokens = \"the nytimes was thrilled by obama talking to communist dictator castro and horrified by trump talking to the elected le\".split(\" \")\n",
    "token_vector = model.infer_vector(word_tokens)\n",
    "sims = model.docvecs.most_similar([token_vector])\n",
    "print(sims)\n",
    "\n",
    "# print out the original text\n",
    "data = pd.read_csv(\"./datasets/13_retweet_text_doc2vec_new.csv\", sep='\\t')\n",
    "print(\"the nytimes was thrilled by obama talking to communist dictator castro and horrified by trump talking to the elected le\")\n",
    "print('\\n', data.loc[183, 'retweet_text'], '\\n')\n",
    "print(data.loc[584, 'retweet_text'], '\\n')\n",
    "print(data.loc[118, 'retweet_text'], '\\n')\n",
    "print(data.loc[455, 'retweet_text'], '\\n\\n')\n",
    "\n",
    "\n",
    "word_tokens = \"incompetence is really the primary threat from the trump administration\".split(\" \")\n",
    "token_vector = model.infer_vector(word_tokens)\n",
    "sims = model.docvecs.most_similar([token_vector])\n",
    "print(sims)\n",
    "\n",
    "# print out the original text\n",
    "data = pd.read_csv(\"./datasets/13_retweet_text_doc2vec_new.csv\", sep='\\t')\n",
    "print('\\n', data.loc[38, 'retweet_text'], '\\n')\n",
    "print(data.loc[569, 'retweet_text'], '\\n')\n",
    "print(data.loc[40, 'retweet_text'], '\\n')\n",
    "print(data.loc[324, 'retweet_text'], '\\n\\n')\n",
    "\n",
    "\n",
    "word_tokens = \"obama turned his back on taiwan\".split(\" \")\n",
    "token_vector = model.infer_vector(word_tokens)\n",
    "sims = model.docvecs.most_similar([token_vector])\n",
    "print(sims)\n",
    "\n",
    "# print out the original text\n",
    "data = pd.read_csv(\"./datasets/13_retweet_text_doc2vec_new.csv\", sep='\\t')\n",
    "print('\\n', data.loc[128, 'retweet_text'], '\\n')\n",
    "print(data.loc[579, 'retweet_text'], '\\n')\n",
    "print(data.loc[134, 'retweet_text'], '\\n')\n",
    "print(data.loc[162, 'retweet_text'], '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./datasets/10_original_text_doc2vec.csv', sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "normal",
   "language": "python",
   "name": "normal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
