{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30567\n",
      "\"@MysticWolf12001: @realDonaldTrump C'mon, you KNOW you're the only one capable of beating Clinton\"\n"
     ]
    }
   ],
   "source": [
    "# f = open('./datasets/6_retweet_text_only.csv', 'r', encoding=\"utf-8\")\n",
    "# tweet = []\n",
    "# for line in f.readlines():\n",
    "#     line = line.strip()\n",
    "#     data = line.split(' ')\n",
    "#     tweet.append(data)\n",
    "# f.close()\n",
    "\n",
    "f = pd.read_csv('./datasets/4_original_text_only.csv', sep='\\t')\n",
    "print(len(f))\n",
    "tweet = f.iloc[:, 0].tolist()\n",
    "print(tweet[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ricardo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30567\n",
      "['\"@MysticWolf12001:', \"C'mon,\", 'one', 'beating', 'Clinton\"']\n"
     ]
    }
   ],
   "source": [
    "# 去除stop words, link, account\n",
    "nltk.download('stopwords')\n",
    "nltk_stopwords = nltk.corpus.stopwords.words('english')\n",
    "result1 = []\n",
    "\n",
    "for i in range(0, len(tweet)):\n",
    "    tweet_split_list = tweet[i].split(\" \")\n",
    "    words = []\n",
    "    for j in range(0, len(tweet_split_list)):\n",
    "        if j == 1:\n",
    "            if '@' not in tweet_split_list[j]:\n",
    "                if 'http' not in tweet_split_list[j]:\n",
    "                    if tweet_split_list[j] not in nltk_stopwords and tweet_split_list[j].lower() not in nltk_stopwords and tweet[i][j].upper() not in nltk_stopwords:\n",
    "                        words.append(tweet_split_list[j])\n",
    "        else:\n",
    "            if tweet[i][j] not in nltk_stopwords and tweet_split_list[j].lower() not in nltk_stopwords and tweet_split_list[j].upper() not in nltk_stopwords:\n",
    "                if 'http' not in tweet_split_list[j]:\n",
    "                    words.append(tweet_split_list[j])\n",
    "    result1.append(words)\n",
    "    \n",
    "print(len(result1))\n",
    "print(result1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30567\n",
      "[['ChadWilliams91', 'Ill', 'move', 'states', 'you', 'are', 'hired', 'Thanks'], ['HunterBalthazor', 'ran', 'president', 'vote', 'back', 'track']]\n"
     ]
    }
   ],
   "source": [
    "# 去除標點符號\n",
    "result2 = []\n",
    "for i in range(0, len(result1)):\n",
    "    words = []\n",
    "    for j in range(0, len(result1[i])):\n",
    "        out = ''.join(c for c in result1[i][j] if c not in string.punctuation)\n",
    "        out = ''.join(x for x in out if ord(x) < 256)\n",
    "        words.append(out)\n",
    "    while '' in words:\n",
    "        words.remove('')\n",
    "    while 'rt' in words:\n",
    "        words.remove('rt')\n",
    "    while '\\uea1d' in words:\n",
    "        words.remove('\\uea1d')\n",
    "    result2.append(words)\n",
    "\n",
    "print(len(result2))\n",
    "print(result2[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30567\n",
      "Index(['is_RT', 'retweet_from', 'whole_tweet_text', 'original_text',\n",
      "       'retweet_text'],\n",
      "      dtype='object')\n",
      "30567\n",
      "   is_RT  retweet_from                                   whole_tweet_text  \\\n",
      "0      0           NaN  \"@MysticWolf12001: @realDonaldTrump C'mon, you...   \n",
      "1      0           NaN  \"@Chad_Williams91: @realDonaldTrump if you're ...   \n",
      "2      0           NaN  \"@HunterBalthazor: @realDonaldTrump if you ran...   \n",
      "3      0           NaN  \"@kyleraccio: @realDonaldTrump @Vinny_Titone I...   \n",
      "4      0           NaN                @HarryCraig96 @TrumpTowerNY Thanks!   \n",
      "\n",
      "                                       original_text  retweet_text  \\\n",
      "0  \"@MysticWolf12001: @realDonaldTrump C'mon, you...           NaN   \n",
      "1  \"@Chad_Williams91: @realDonaldTrump if you're ...           NaN   \n",
      "2  \"@HunterBalthazor: @realDonaldTrump if you ran...           NaN   \n",
      "3  \"@kyleraccio: @realDonaldTrump @Vinny_Titone I...           NaN   \n",
      "4                @HarryCraig96 @TrumpTowerNY Thanks!           NaN   \n",
      "\n",
      "                     stopwords_removed_original_text  \n",
      "0           MysticWolf12001 Cmon one beating Clinton  \n",
      "1  ChadWilliams91 Ill move states you are hired T...  \n",
      "2      HunterBalthazor ran president vote back track  \n",
      "3  kyleraccio VinnyTitone think hell lead polls l...  \n",
      "4                                       HarryCraig96  \n"
     ]
    }
   ],
   "source": [
    "# f = open('./datasets/7_retweet_text_only_stopwords_removed.csv', \"w+\", encoding=\"utf-8\")\n",
    "# for i in range(0, len(result2)):\n",
    "#     print(result2[i])\n",
    "#     s = \"\"\n",
    "#     for j in range(0, len(result2[i])):\n",
    "#         s += result2[i][j] + ' '\n",
    "#     f.write(s)\n",
    "#     f.write('\\n')\n",
    "# f.close()\n",
    "\n",
    "content_list = []\n",
    "for i, j in enumerate(result2):\n",
    "    content = \" \".join(str(b) for a, b in enumerate(j))\n",
    "    content_list.append(content)\n",
    "\n",
    "# read in the data with whole information\n",
    "original_data = pd.read_csv(\"./datasets/3_original_text.csv\", sep='\\t')\n",
    "print(len(original_data))\n",
    "print(original_data.columns)\n",
    "\n",
    "# add new column\n",
    "# original_data.loc[:, 'stopwords_removed_retweet_text'] = content_list\n",
    "original_data.loc[:, 'stopwords_removed_original_text'] = content_list\n",
    "print(len(original_data))\n",
    "print(original_data.head())\n",
    "\n",
    "original_data.to_csv(\"./datasets/8_original_text_only_stopwords_removed.csv\", \n",
    "                     sep=\"\\t\",\n",
    "                    index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
